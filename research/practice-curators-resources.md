# Practice/Competence Curator Business Models — Resource Catalog

**Track**: Practice Curators — Organizations that curate best practices from practitioner networks
**Round 1** | 2026-02-14 | **Round 2** | 2026-02-14

---

## 1. APQC (American Productivity & Quality Center)

- **URL**: https://www.apqc.org
- **Type**: Nonprofit membership-based research and benchmarking organization
- **Founded**: 1977
- **Description**: APQC is the closest pure example of the "practice curator" model. It operates a network of member organizations, collects their operational data and practices through structured benchmarking, and synthesizes this into curated best practice reports, frameworks, and knowledge bases. Members submit data (e.g., process metrics, knowledge management practices) and in return get access to benchmarks, best practice studies, and advisory services. APQC does not consult — it curates what works from across its membership and makes the distilled knowledge available to all members.
- **Who they serve**: Large enterprises across industries — primarily process improvement, knowledge management, and operational excellence leaders
- **How they collect/curate**: Structured benchmarking surveys, site visits to "best practice partner" organizations, member-submitted case studies, research analysts who synthesize patterns across member data
- **Revenue model**: Tiered annual membership (estimated USD 15K-100K+ depending on tier and organization size). Higher tiers get more advisory access and custom benchmarking. Also sells individual reports and training.
- **Network size**: ~500 member organizations including many Fortune 500 companies
- **What makes it work**: The flywheel — members contribute data to get benchmarks, which makes the benchmarks more valuable, which attracts more members. APQC's neutrality (nonprofit, vendor-agnostic) builds trust. Members share openly because APQC anonymizes and aggregates.

| Dimension | Relevance to Agents 102 |
|-----------|------------------------|
| Practice curation from network | 5 — Core model match |
| Peer network as product | 4 — Network exists but benchmarking is the primary value, not peer interaction |
| Original research | 3 — Research is secondary to member data curation |
| Training as entry point | 2 — Training exists but is not the primary engagement model |
| Revenue via membership | 5 — Direct match |

**Key insight for Agents 102**: APQC proves the "curate from members, sell back to members" flywheel works at scale. But APQC is dry — benchmarking reports, not community energy. Agents 102 could learn the curation mechanics while adding the community warmth that APQC lacks.

---

## 2. Gartner

- **URL**: https://www.gartner.com
- **Type**: Public company (NYSE: IT) — research and advisory
- **Founded**: 1979
- **Description**: Gartner is the largest practice-adjacent research firm, but with an important distinction: it combines practitioner data with analyst-driven research, rather than curating practices directly from a member network. Gartner surveys thousands of practitioners, interviews CIOs and technology leaders, and synthesizes patterns into frameworks (Magic Quadrants, Hype Cycles, maturity models). Its peer networking happens through Gartner Peer Community and conferences, but the primary product is analyst opinion informed by practitioner data — not pure practice curation.
- **Who they serve**: IT leaders, CIOs, technology executives, vendor companies
- **How they collect/curate**: Analyst-conducted research, practitioner surveys (tens of thousands annually), vendor briefings, Gartner Peer Community (online peer forum), conferences with practitioner talks
- **Revenue model**: Annual subscription to research (USD 30K-100K+ per seat for enterprise). Conferences (USD 3K-10K per attendee). Consulting engagements. Total revenue ~USD 6 billion.
- **Network size**: 15,000+ client organizations, 2,000+ analysts
- **What makes it work**: Gartner's authority is self-reinforcing — because everyone reads Gartner, being in a Gartner report matters, which means vendors and practitioners cooperate with Gartner research, which makes the research more comprehensive. The "analyst as curator" model scales better than pure peer-network curation because it does not depend on member willingness to share.

| Dimension | Relevance to Agents 102 |
|-----------|------------------------|
| Practice curation from network | 3 — Analyst-mediated, not direct peer curation |
| Peer network as product | 3 — Peer Community exists but is secondary to analyst research |
| Original research | 5 — Primary output is research and frameworks |
| Training as entry point | 2 — Conferences serve this role somewhat |
| Revenue via membership | 5 — Subscription-based |

**Key insight for Agents 102**: Gartner shows that "curated practitioner intelligence" is a multi-billion-dollar market. But Gartner is analyst-driven — practitioners are data sources, not co-creators. Agents 102's model is more participatory: network members co-create the practices, not just supply data. This is closer to APQC than Gartner.

---

## 3. Vistage / YPO (Young Presidents' Organization)

- **URL**: https://www.vistage.com / https://www.ypo.org
- **Type**: Peer advisory networks for business leaders
- **Founded**: Vistage 1957, YPO 1950
- **Description**: Vistage and YPO are peer network organizations where business leaders meet in small groups (15-20) to share challenges, practices, and advice. A trained chair/moderator facilitates each group. The core product is the peer group itself — curated, confidential, and structured. Best practices emerge organically from member discussions rather than being formally collected and published. Vistage adds executive coaching and speaker events. YPO adds global networking and family/legacy programming.
- **Who they serve**: CEOs, business owners, and C-suite executives (Vistage: companies USD 1M-1B revenue; YPO: requires leading a company meeting certain revenue/employee thresholds)
- **How they collect/curate**: Monthly peer group meetings with facilitated discussion. Members share challenges and solutions. The "curation" is informal — it happens through conversation, not formal knowledge management. Vistage publishes some research but it is secondary to the group experience.
- **Revenue model**: Annual membership fees. Vistage: ~USD 15K-25K/year. YPO: ~USD 5K-10K/year plus chapter dues. Vistage also pays chairs (facilitators) who are independent contractors.
- **Network size**: Vistage: ~45,000 members globally. YPO: ~35,000 members in 150+ countries.
- **What makes it work**: Confidentiality and trust. Small group intimacy. The "lonely at the top" problem — CEOs cannot discuss real challenges with their teams, so they need a peer group. Facilitation quality is critical. Selectivity (not everyone can join) creates perceived value.

| Dimension | Relevance to Agents 102 |
|-----------|------------------------|
| Practice curation from network | 2 — Practices shared informally, not formally curated or published |
| Peer network as product | 5 — The peer group IS the product |
| Original research | 1 — Research is a minor add-on |
| Training as entry point | 2 — Events and speakers serve this role |
| Revenue via membership | 5 — Pure membership model |

**Key insight for Agents 102**: Vistage/YPO prove that "the network is the product" works at massive scale — but they do not curate or publish practices. The knowledge stays inside the group. Agents 102 wants the network dynamic of Vistage PLUS the curation output of APQC. This combination is rare.

---

## 4. CNCF / Linux Foundation (Open-Source Foundation Model)

- **URL**: https://www.cncf.io / https://www.linuxfoundation.org
- **Type**: Nonprofit open-source foundations
- **Founded**: CNCF 2015, Linux Foundation 2000
- **Description**: The Cloud Native Computing Foundation (CNCF) is a practice curator for the cloud-native ecosystem. Member companies (Google, Microsoft, Red Hat, etc.) contribute projects, practices, and expertise. CNCF curates this into a structured landscape (the CNCF Landscape), maturity levels (sandbox/incubating/graduated), and best practice guides. Training and certification (CKA, CKAD) serve as the entry point and revenue stream, while the curated landscape and project governance are the core value. The Linux Foundation operates similarly at a broader scale across dozens of sub-foundations.
- **Who they serve**: Cloud-native technology practitioners and the companies that employ them
- **How they collect/curate**: Member companies contribute open-source projects. CNCF's Technical Oversight Committee evaluates and organizes them into maturity levels. End User Technical Advisory Board collects real-world deployment practices. Annual surveys collect adoption data. KubeCon conferences gather community knowledge.
- **Revenue model**: Membership dues (USD 5K for small companies to USD 370K+ for platinum members). Training and certification (USD 395 per exam, estimated tens of millions annually). Conference revenue (KubeCon: 10,000+ attendees at USD 1K-2K each). Total Linux Foundation revenue ~USD 250M+.
- **Network size**: CNCF: 800+ member companies, 180+ hosted projects, 200,000+ individual contributors. Linux Foundation: 2,000+ member organizations.
- **What makes it work**: The "commons" model — companies contribute because the shared infrastructure benefits everyone. CNCF adds curation (the landscape, maturity levels) that individual companies cannot create alone. Training and certification create a talent pipeline that makes the ecosystem more valuable. Governance neutrality (no single company controls CNCF) enables trust.

| Dimension | Relevance to Agents 102 |
|-----------|------------------------|
| Practice curation from network | 5 — Core function: curate what works from the ecosystem |
| Peer network as product | 4 — Community is strong but secondary to curated landscape |
| Original research | 3 — Surveys and reports, but primarily curates member contributions |
| Training as entry point | 5 — Training and certification are major revenue and engagement drivers |
| Revenue via membership | 5 — Membership + training + events |

**Key insight for Agents 102**: CNCF is the strongest structural analog. Training is the entry point. Curation of member practices is the core product. The landscape/maturity model creates a navigable map of a complex ecosystem. Agents 102 could create an equivalent "agentic practice landscape" — curated from network members' real deployments, organized by maturity/domain, and continuously updated.

---

## 5. IHI (Institute for Healthcare Improvement)

- **URL**: https://www.ihi.org
- **Type**: Nonprofit focused on healthcare quality improvement
- **Founded**: 1991
- **Description**: IHI operates a model that closely matches the Agents 102 vision. It runs improvement networks (called "collaboratives") where healthcare organizations work together on specific quality challenges. IHI curates the practices that emerge from these collaboratives into "improvement packages" — bundles of evidence-based practices that other organizations can adopt. IHI also conducts its own research, runs training programs (the IHI Open School), hosts conferences (the IHI Forum), and publishes white papers. The genius of the IHI model: training creates practitioners, practitioners join collaboratives, collaboratives generate practices, practices become training content. It is a closed loop.
- **Who they serve**: Healthcare organizations — hospitals, health systems, public health agencies, globally
- **How they collect/curate**: Improvement collaboratives (multi-org learning networks focused on specific topics). Participating organizations test practices, report results, and IHI synthesizes what works into "change packages" and "bundles." These are then published and taught globally.
- **Revenue model**: Mixed — conference revenue, training fees (Open School), consulting/advisory, collaborative participation fees, grants and philanthropy. Total revenue ~USD 60-80M.
- **Network size**: IHI has engaged organizations in 60+ countries. The IHI Open School has reached over 9 million learners. Collaboratives typically involve 20-100 organizations each.
- **What makes it work**: The improvement science methodology provides rigor. The "bundle" concept (a curated set of practices that work together) is a powerful packaging innovation — it is not just a list of best practices, it is a tested combination. IHI's mission-driven nature (saving lives) creates emotional commitment that commercial networks cannot match.

| Dimension | Relevance to Agents 102 |
|-----------|------------------------|
| Practice curation from network | 5 — Core function: collaboratives generate and curate practices |
| Peer network as product | 5 — Collaboratives are multi-org learning networks |
| Original research | 4 — Improvement science research informs and validates practices |
| Training as entry point | 5 — IHI Open School is a massive training funnel |
| Revenue via membership | 4 — Not pure membership, but collaborative participation fees + training revenue |

**Key insight for Agents 102**: IHI is the most complete model match. The "training creates practitioners, practitioners join collaboratives, collaboratives generate practices, practices become training content" loop is exactly what Agents 102 envisions. The "bundle" concept (a curated, tested combination of practices) is a powerful product format that Agents 102 could adapt as "agentic practice packs."

---

## 6. IHI Breakthrough Series Collaborative — Operational Deep Dive (Round 2)

- **URL**: https://www.ihi.org/library/white-papers/breakthrough-series-ihis-collaborative-model-achieving-breakthrough
- **Also see**: https://diabetesjournals.org/spectrum/article/17/2/97/2067/The-Breakthrough-Series-IHI-s-Collaborative-Model and https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2677044/
- **Type**: The operational unit inside IHI that generates and curates practices — the engine of the flywheel

**How a Breakthrough Series Collaborative (BTS) actually works:**

1. **Topic selection**: IHI identifies a clinical or operational area where a large gap exists between what is known (evidence-based best practice) and what is actually done in most organizations. The gap must be large enough that closing it would produce measurable, significant improvement.

2. **Expert faculty and change package development**: IHI assembles a faculty of 5-15 subject-matter experts — clinicians, researchers, and improvement practitioners who have already achieved results in the topic area. This faculty develops the "change package" — a curated set of evidence-based changes that, when implemented together, are expected to produce breakthrough improvement. The change package is not a vague list of recommendations; it is a specific, actionable set of practices with defined measures.

3. **Enrollment**: IHI recruits 20-100 organizations to participate. Each organization sends a team of ~3 people (a senior leader/sponsor, a clinical expert, and a day-to-day improvement lead). Participating organizations pay fees to join the collaborative.

4. **Pre-work**: Before the first Learning Session, teams conduct assessments of their current state against the change package measures.

5. **Learning Session 1** (multiday, face-to-face): Expert faculty present the change package. Teams learn improvement science methodology (PDSA cycles — Plan-Do-Study-Act). Teams set aims and develop initial change plans. Peer-to-peer sharing begins.

6. **Action Period 1** (months, in the field): Teams return to their organizations and begin testing changes from the change package using small-scale PDSA cycles. They measure results and report monthly via extranet. Knowledge sharing continues through monthly conference calls, team-to-team phone calls, listserv discussions, and written monthly reports posted to the collaborative extranet.

7. **Learning Session 2** (multiday, face-to-face): Teams present their results — what worked, what did not. Peer learning is the primary activity. Faculty revise the change package based on what teams are discovering. New practices that emerge from teams are incorporated. High-performing teams teach others.

8. **Action Period 2**: More testing, wider implementation within organizations.

9. **Learning Session 3** (multiday, face-to-face): Final sharing of results. Celebration of achievements. Planning for sustainability and spread beyond the collaborative.

10. **After the collaborative**: IHI distills the tested change package — now refined by 20-100 organizations' real-world experience — into published "change packages," "bundles," and white papers. These become the input for IHI training programs, Open School courses, and future collaboratives.

**Key operational mechanics:**

- **Monthly performance data posting**: Every team posts their performance metrics to a shared extranet monthly. This creates transparency, peer pressure, and a real-time learning dataset.
- **All-teach, all-learn**: The model explicitly positions every team as both a teacher and a learner. No one is just receiving.
- **PDSA as the common language**: The Plan-Do-Study-Act cycle gives every team a shared methodology for testing changes, which makes results comparable across organizations.
- **The change package evolves**: The initial change package is a hypothesis, not a prescription. It is explicitly revised during the collaborative based on what teams discover. By the end, the change package reflects the collective intelligence of all participating organizations — not just the expert faculty's initial knowledge.

**What participants value most** (from PMC research): Collaborative expert faculty and the change package were rated among the most helpful features. Peer-to-peer sharing between teams was also highly valued — sometimes more than expert input, because peers face similar constraints.

**Scale and impact**: IHI has run hundreds of BTS collaboratives since 1995. The model has been replicated globally — by government health agencies, NGOs, and other improvement organizations. It has been applied to patient safety, chronic disease management, hospital operations, and public health.

**What fails**: Collaboratives fail when (a) organizational leadership does not support the team, (b) teams do not test changes between Learning Sessions and show up empty-handed, (c) the topic is too broad or the change package is too vague, (d) measurement is not rigorous enough to show whether changes actually worked.

| Dimension | Relevance to Agents 102 |
|-----------|------------------------|
| Practice collection mechanics | 5 — Monthly data reporting, PDSA cycles, team presentations at Learning Sessions |
| Practice curation/distillation | 5 — Expert faculty + team experience = refined change package |
| Practice distribution | 5 — Published change packages, bundles, training content, Open School |
| Replicability for Agents 102 | 5 — The BTS is the operational blueprint for "agentic practice collaboratives" |

**Key insight for Agents 102**: The BTS model is the operational playbook. Replace "clinical quality gap" with "agentic capability gap." Replace "change package" with "agentic practice pack." Replace "PDSA cycles" with "build-measure-learn sprints." The structure — expert faculty, change package, Learning Sessions, Action Periods, monthly data sharing, refined output — translates directly. The 6-15 month timeframe is especially relevant: it is long enough to generate real results, short enough to maintain urgency.

---

## 7. Carnegie Foundation — Networked Improvement Communities (NICs)

- **URL**: https://www.carnegiefoundation.org/improvement-in-education/
- **Also see**: https://www.carnegiefoundation.org/blog/why-a-nic/ and https://www.carnegiefoundation.org/blog/five-essential-building-blocks-for-a-successful-networked-improvement-community/
- **Type**: Nonprofit foundation applying improvement science to education
- **Founded**: 1905 (Carnegie Foundation), NICs developed ~2010s
- **Description**: The Carnegie Foundation for the Advancement of Teaching developed the Networked Improvement Community (NIC) model to address persistent problems in education. NICs are intentionally designed communities of practitioners, researchers, and community members that unite around a common problem of practice. They use improvement science (the same PDSA methodology IHI uses) to iteratively test and refine practices. Carnegie provides the hub leadership, methodology, and convening — member organizations provide the on-the-ground testing and adaptation. The model explicitly acknowledges that most education interventions fail not because the idea is bad, but because context varies — so practices must be tested across diverse contexts and adapted, not just copied.
- **Who they serve**: Education systems — school districts, community colleges, university programs, education nonprofits
- **How they collect/curate**: NIC members test changes using PDSA cycles in their local contexts. Results are shared across the network. The hub (Carnegie or a designated lead organization) synthesizes what works, identifies variation by context, and publishes practical guidance. Five essential building blocks: (1) a well-specified common aim, (2) a shared understanding of the problem and its causes, (3) a shared theory of improvement with measurable targets, (4) disciplined use of improvement science methods, (5) hub leadership that coordinates, synthesizes, and maintains network coherence.
- **Revenue model**: Foundation-funded (Carnegie's endowment). Some NICs are supported by government grants (e.g., IES — Institute of Education Sciences). Not a commercial model — but the mechanics are transferable.
- **Network size**: NICs vary — typically 10-50 organizations per network. Carnegie has run several high-profile NICs including the Community College Pathways NIC (dozens of colleges focused on developmental math) and Building a Teaching Effectiveness Network (BTEN).
- **What makes it work**: The "hub leadership" role is critical — someone must actively coordinate, synthesize, and push the network forward. Without strong hub leadership, NICs devolve into loose information-sharing groups with no curation output. The common aim must be specific and measurable, not aspirational. Improvement science discipline prevents the network from becoming a talking shop.

| Dimension | Relevance to Agents 102 |
|-----------|------------------------|
| Practice curation from network | 5 — Core function: test practices across contexts, synthesize what works |
| Peer network as product | 4 — Network is the engine, but published practices are the output |
| Original research | 4 — Improvement research integrated into the curation process |
| Training as entry point | 3 — Training in improvement science methods is embedded, but not a separate revenue stream |
| Revenue via membership | 2 — Foundation/grant-funded, not commercial |

**Key insight for Agents 102**: Carnegie's NIC model adds a crucial dimension that IHI's BTS model does not emphasize as explicitly: **context variation**. Carnegie explicitly designs NICs to test practices across diverse contexts (urban vs. rural schools, community colleges vs. universities) and to understand *where* a practice works, not just *whether* it works. For Agents 102, this maps to testing agentic practices across different company sizes, industries, and maturity levels — and curating context-specific guidance, not one-size-fits-all recommendations. Carnegie's "five essential building blocks" are also a useful checklist for designing Agents 102 collaboratives.

---

## 8. NIST MEP (Manufacturing Extension Partnership) + Toyota Kata Community

- **URL**: https://www.nist.gov/mep and https://en.wikipedia.org/wiki/Toyota_Kata
- **Type**: Government-sponsored manufacturing improvement network (MEP) + practitioner improvement methodology community (Toyota Kata)
- **Founded**: MEP 1988, Toyota Kata codified 2009 (Mike Rother's research)
- **Description**: The MEP National Network is a public-private partnership operating through ~50 centers across all 50 US states. Each center works directly with small and medium-sized manufacturers (SMMs) to help them adopt best practices — lean manufacturing, Toyota Kata, quality systems, workforce development. MEP centers act as practice distributors and coaches, not just trainers. They collect successful implementation patterns from their client engagements and share them across the network via NIST coordination. Toyota Kata, which many MEP centers teach, provides a structured practice for continuous improvement: the Improvement Kata (a scientific thinking pattern for navigating from current state to target condition) and the Coaching Kata (a pattern for teaching the Improvement Kata to others). The Kata community has grown organically through practitioners who share experiences, exercises, and adaptations.
- **Who they serve**: Small and medium-sized manufacturers (typically 5-500 employees)
- **How they collect/curate**: MEP centers report outcomes and successful practices to NIST, which aggregates impact data and disseminates effective approaches across the network. The Toyota Kata community shares through conferences (KataCon), online communities, published case studies, and a network of certified Kata coaches. The Kata methodology itself is the "change package" — a standardized improvement pattern that practitioners adapt to their context.
- **Revenue model**: MEP centers receive federal funding (~USD 160M/year from NIST), state/local matching funds, and fee-for-service revenue from client engagements. The Toyota Kata community is largely volunteer/practitioner-driven, with revenue from books, conferences, and coaching services. Total MEP network economic impact reported at ~USD 18B annually in new sales across client manufacturers.
- **Network size**: MEP: ~50 centers, ~1,400 staff, serving ~35,000 manufacturers annually. Toyota Kata community: thousands of practitioners globally, loosely connected.
- **What makes it work**: MEP's distributed center model puts practice coaches physically close to manufacturers — improvement happens on the shop floor, not in a webinar. Toyota Kata works because it is a *meta-practice* — it does not prescribe what to improve, but teaches a rigorous pattern for how to improve anything. This makes it domain-agnostic and infinitely adaptable.

| Dimension | Relevance to Agents 102 |
|-----------|------------------------|
| Practice curation from network | 3 — MEP aggregates at network level but curation is less formalized than IHI |
| Peer network as product | 3 — MEP is hub-and-spoke (center-to-manufacturer), not peer-to-peer |
| Original research | 2 — NIST provides research backing but MEP is primarily a delivery network |
| Training as entry point | 4 — Training and coaching are the primary engagement model |
| Revenue via membership | 2 — Government-funded, not membership-based |

**Key insight for Agents 102**: Two transferable ideas. First, Toyota Kata demonstrates the power of a **meta-practice** — teaching people *how to improve* rather than *what to do*. Agents 102 could embed a similar "agentic improvement kata" — a structured pattern for how organizations discover, test, and adopt agentic practices. This is more durable than any specific practice pack because it works even as technology shifts. Second, MEP shows that **distributed local coaching** dramatically increases adoption. Practices that are taught in a classroom get forgotten; practices that are coached on-site get implemented. This suggests Agents 102 should invest in facilitator/coach capability, not just training content.

---
