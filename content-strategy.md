# Agents 102 — Content Strategy

## The Problem

Everyone says agents will be the big thing. The press is full of it. Vendors are selling it. Boards are asking about it. McKinsey, Deloitte, IBM — they've all published frameworks for the "agentic enterprise."

But here's what nobody says out loud: **without agent competence, there is no vision.** The consultancy frameworks are governance of an abstraction. Leadership reads the McKinsey report, approves a governance framework, launches an initiative portfolio — and 18 months later, nobody in the room has ever seen an agent work with their company's data.

Previous transformations — digital, agile, cloud — could be led from strategy alone. An executive didn't need to build software to envision a digital company. The agentic transformation is different. You cannot envision what agents do for your organization if you cannot think agents. Without that competence, the frameworks are theatre. Just governance of abstract. Just lists of initiatives. No felt vision.

The gap is not between strategy and execution. The gap is between strategy and competence.

## Who Moves First

The builder leader. The CEO, CTO, SVP of HR who wants to *own* the transformation — not outsource it to a consultancy. This is a psychographic, not a title. It's the leader who is personally frustrated with transformation theatre: governance frameworks that govern nothing real, initiative lists with no vision behind them, quarterly reviews that produce slides but no felt change.

The builder leader wants their organization to *have* the competence, not rent it from a consultancy. They understand that when the consultancy leaves, the slides go stale — but when people learn to think agents, the capability compounds.

They need to look outward at the super early adopters. These aren't people waiting for enterprise platforms. They're using Claude Code, ChatGPT, Codex — automating their lives and businesses right now. The things running in Claude Code today? Those are agents. Real ones, solving real problems.

Once the builder leader gets it, they can move. And once they move — with their own structure, their own policies, their own people — the path appears. The strategy emerges from the action.

## The Scaling Challenge

You cannot train everyone at €1000/head. It's too expensive and too slow.

**Teach everyone to think. Scale for future. With your own structure.**

SAFe got the bias to action right ("launch trains, train everyone") but the rigidity wrong — SAFe's trains, SAFe's structure, SAFe's certification. The company adapted to SAFe, not SAFe to the company. We take the action bias and fix the ownership.

The answer is a five-step journey:

1. **Bootstrap** — €500/person, 2-day intensive. Anyone can walk in. You build a real agent. The vision emerges from action.
2. **Make Your Own** — a few key change leaders design the rollout with mentoring. Change strategy lab. Not just "we learned agents" — "we have a plan to transform."
3. **Champions** — the best from Bootstrap learn to facilitate and adapt the framework. Train-the-trainer. The multiplier.
4. **Scale** — champions run it inside their org, with their own structure, at their own pace. Zero external dependency.
5. **Drive into Value Chains** — where do agents create real business value? Map your value chains. Rework them with agents. Learn from peers who've already done it.

All materials are **co-branded with the customer and co-copyright for further development.** The org doesn't license our IP — they co-own it. Anti-SAFe by design: no lock-in, no certification dependency. If you stop working with us, you keep everything.

The training doesn't just teach. It creates people who can multiply the understanding across their organization. Every graduate should be able to explain agents without jargon, show a real example, guide someone through their first experience, and make the case for where agents create value in their domain.

Framework + plug points. Their policies, their systems, their context. Not our template — their reality.

## What We Teach

The fundamentals of making agents with LLMs.

There is no ready-made platform that just enables agent creation at scale. Not yet. But the fundamentals are teachable, durable, and transferable:

- What an LLM actually does (demystified, no jargon)
- How you go from "a chatbot" to "an agent" — what changes, mechanically
- What tools, memory, and reasoning mean in practice
- How to give an agent a task, boundaries, and judgment
- What early adopters are doing right now with real tools

This is fundamentals-first, not tool-first. Tools will change. Platforms will mature. The person who understands how LLMs power agents will adapt to whatever comes next.

## The Undertone: Embracing Uncertainty

Most AI training sells certainty: five steps, a roadmap, a framework. That's comforting but dishonest.

This training is honest: **we don't know exactly where this is going.** Nobody does. The future will be lots of agents, created by lots of people and teams, in ways we can't fully predict.

But we do know that agents are being built right now, by real people, solving real problems. The future will be shaped by those who start moving — experimenting, learning, adapting — even without a complete map.

The skill isn't predicting the future. It's learning to move through uncertainty.

This honesty builds trust. The leaders in the room know that anyone promising certainty is lying.

## The Storyline

The training follows a natural arc — not a thick curriculum, but a storyline. Smart people asking good questions about the future, one question leading to the next.

**Act 1: "I can create something"**
The empowerment moment. You sit down with Claude Code and make an agent that does something real. Skeptics become believers. The personal "I did that" feeling.

**Act 2: "How do I scale it?"**
The natural next question. I made one agent for myself — how does this become something a team, a department, an organization can use? The multiplier problem.

**Act 3: "How do I control it?"**
The serious questions arrive. Security. Reliability. Fabrication. Trust. Lifecycle. The things that separate a toy from something you'd stake your reputation on.

**Act 4: The deep mechanics — and the flywheel**
Now it gets methodical and visionary at the same time:
- **Memory** — LLMs that retain and build context across interactions
- **Self-introspection** — LLMs that can reflect on and check their own outputs
- **Continuous improvement** — agents that get better over time
- **The flywheel** — agents creating agents. The ultimate scaling mechanism. Not humans building each agent by hand, but agents that design, build, and improve other agents.

## Why Existing Approaches Fall Short — and What Actually Has a Flywheel

The traditional agent tools and frameworks all share the same problem: they are linear. You build an agent, it does its thing. Done. No compounding.

- **LangChain, CrewAI, AutoGen** — developer frameworks for building agents. Powerful, but each agent is a one-off. You build it, it runs, that's it.
- **n8n, Make, Power Automate** — workflow engines where you can plug AI into individual steps. Great for automation. But the output is a workflow, not more capability.
- **ChatGPT conversations** — every chat is an island. No continuity, no compounding.

None of these have a ramp-to-stars. They are tools that produce output, not tools that produce more tools.

**The secret insight: code-generating agents are the flywheel class.**

Claude Code, ChatGPT Codex, and their successors are fundamentally different. They are agents that build agents. The output of using a code-generating agent is not just a result — it is more capability. One person with Claude Code can create an agent that creates agents. The thing you learn to use is the thing that makes everything else.

This is the ramp-to-stars that no traditional agent framework has:
- **Traditional agent**: you build it → it runs → done.
- **Code-generating agent**: you build with it → it builds agents → those agents create value → you need more agents → you already know how to build them → the capability compounds.

The flywheel is not a design pattern you bolt onto existing tools. It is inherent in the tool class. Code-generating agents are meta-tools: tools that make tools. Once a person learns to think with a code-generating agent, every problem becomes solvable — not because the agent knows the answer, but because the agent can build the thing that finds it.

This is why Bootstrap uses Claude Code specifically. We are not teaching people to use an agent. We are teaching them to use the agent that makes all other agents. The consultancy frameworks (McKinsey, Deloitte, IBM) miss this entirely because they think about deploying agents, not about the meta-capability of building agents with agents.

**What we teach is not "use this tool." It is "understand which class of tool compounds — and learn to think with it."**

## A Remark on What's Ahead

We have only seen the tip of the iceberg of what large language models can do. The storyline of this training is deliberately light — not thick, not textbook-heavy. It rolls forward naturally, driven by curiosity: smart people asking good questions about a future that nobody fully understands yet.

The depth is in the doing, not in the reading.

## The Value Proposition

**Teach everyone to think. Scale for future. With your own structure.**

Three things: **Train → Curate → Connect.**

**Train:** We are the prerequisite for having a real agentic vision — not a governance slide deck. The consultancy frameworks are theatre without agent competence. We create the competence that makes the vision real. Starting at €500/person for a 2-day intensive — a steal compared to consulting, and you get 20 capable people instead of one deliverable.

**Curate:** We track the frontier and know what works and what doesn't. 80% from the best public sources, 20% from the peer network's specific experience. No vendor tracks the full frontier. No consultant has 30 orgs' deployment data. We synthesize both.

**Connect:** The peer network is the long-term product. Companies doing the same transformation, learning from each other. Not structured data collection — peer forums designed for how executives naturally share.

All materials co-branded, co-copyright. The org co-owns everything for further development. No lock-in. No certification dependency.

Not by handing them a finished system. Not by promising a roadmap that doesn't exist. But by giving them the fundamentals, showing them what's real today, connecting them to peers doing the same, and equipping them to keep navigating as this evolves.

Honest confidence, not false certainty. When you start walking, the path appears.
