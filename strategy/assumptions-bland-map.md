# Assumption Map â€” Agents 102 Business Model

Applying David J. Bland's assumption mapping method (Testing Business Ideas / Strategyzer) to the Agents 102 business model canvas.

**Date:** 2026-02-15
**Inputs:** Business model canvas, six research tracks (direct competitors, e-learning benchmarks, practice curators, AI network curators, Helsinki consultancies, AI transformation frameworks).

---

## Desirability Assumptions

These are the highest-uncertainty assumptions. Grouped by theme.

---

### Theme 1: Buyer Exists and Is Reachable

**D1. The "builder leader" persona exists in sufficient numbers**

- **Assumption:** There is a meaningful population of CEOs/CTOs/SVPs who want to *own* AI agent transformation internally rather than outsource it to a consultancy -- and who will invest budget in building internal capability rather than buying advisory.
- **Importance:** Critical
- **Evidence:** Weak
- **Current basis:** The persona is defined psychographically ("the leader who led their agile/digital transformation personally"). The research validates that consultancies do not offer training products (competitors-direct Round 3) and that frameworks all say "upskill your workforce" without providing the means (ai-transformation-frameworks). But we have no direct evidence that this buyer persona exists in the volumes needed. The persona is constructed from reasoning, not from buyer conversations.
- **What would disprove it:** If early outreach finds that most leaders prefer to hire a Solita or Reaktor to "do AI for us" rather than build the capability internally. If the response to the pitch is "we already have an AI partner" rather than "yes, we need our own people to learn this."
- **Test idea:** 10 structured discovery interviews with Finnish/Nordic C-suite and SVP-level leaders. Ask about their current AI approach, whether they feel dependent on external partners, and whether they would invest in building internal agentic competence. Do not pitch -- listen for the pain.

---

**D2. The buyer distinguishes Agents 102 from existing training options**

- **Assumption:** When a Finnish SVP of HR or CDO decides to invest in AI capability-building, they will see Agents 102 as meaningfully different from (a) Aalto EE's AI leadership programs, (b) vendor training from Microsoft/Salesforce, and (c) embedded consulting-training from Solita/Reaktor.
- **Importance:** Critical
- **Evidence:** None
- **Current basis:** The competitive research shows a genuine structural gap -- no program combines hands-on building + strategic framing + non-coder accessibility + organizational scope (competitors-direct). But "structural gap" is an analyst's observation, not a buyer's perception. The buyer may not see the dimensions we see. Aalto EE may feel "good enough" (helsinki-consultancies Round 2). The buyer might say "we already do AI training" without distinguishing awareness-level from capability-building.
- **What would disprove it:** If buyers describe their existing AI training (Aalto EE, vendor courses, consulting workshops) as sufficient for building agentic capability. If they do not perceive a gap between "understanding AI" and "being able to build and deploy agents."
- **Test idea:** In discovery interviews, ask what AI training they have done already and what is missing. If they say "nothing is missing" or describe needs that Aalto EE / vendor training already covers, the assumption fails.

---

**D3. The buyer is reachable through our channels**

- **Assumption:** We can reach the builder leader through direct engagement, conferences, and pilot cohorts -- without needing an established institutional brand or existing enterprise L&D relationships.
- **Importance:** High
- **Evidence:** Weak
- **Current basis:** The business model canvas lists direct engagement, conference workshops, and pilot cohorts as channels. The Helsinki research (Round 2) reveals that Aalto EE owns the Finnish L&D buyer relationship and Solita/Reaktor own the "who do we call about AI" relationship. A new entrant without these relationships faces a cold-start problem. Nordic Business Forum is identified as a lead-generation venue but that is speculative.
- **What would disprove it:** If conference workshop slots are gated by institutional reputation. If direct outreach to enterprise L&D buyers gets no response without a recognized brand. If the buyer's default behavior is to call Aalto EE or their existing consultancy.
- **Test idea:** Attempt 20 cold outreach messages to Finnish/Nordic enterprise L&D and digital transformation leaders. Measure response rate and willingness to take a 30-minute discovery call. Also: apply to speak at 2-3 Nordic conferences. If both fail, the channel assumption is wrong and a partnership model (e.g., Aalto EE co-branding) becomes necessary.

---

**D4. Mid-market and enterprise companies will buy agent training for non-technical staff**

- **Assumption:** Organizations will allocate L&D budget specifically for training non-coders (product managers, analysts, team leads, operations staff) in agent building -- not just for developers or for executive awareness.
- **Importance:** Critical
- **Evidence:** Partial
- **Current basis:** IBM launched an agentic AI professional certificate, signaling enterprise L&D budget movement (e-learning Round 1). Gartner predicts 40% of enterprise apps will have agents by end of 2026 (ai-network-curators). Pluralsight has agent content in enterprise catalogs (e-learning Round 3). But all of these serve either developers or executives -- not the "non-technical builder" we target. We have no evidence that L&D buyers see "non-coders building agents" as a legitimate training category worth budget.
- **What would disprove it:** If L&D buyers say "our developers handle agent building" or "our people just need to understand AI at a strategic level -- they don't need to build anything." If the "non-coder builder" framing feels illegitimate to enterprise buyers.
- **Test idea:** In discovery interviews, describe the learning outcome ("your product managers and operations leads will build working agents with your company's actual policies and data") and gauge reaction. Does this feel valuable or strange? Is this a budget category they recognize?

---

### Theme 2: Value Proposition Resonates

**D5. "Competence is the prerequisite for vision" resonates with buyers**

- **Assumption:** The core narrative -- that executives cannot envision an agentic organization without first building hands-on agent competence, and that consultancy frameworks are "governance of an abstraction" without it -- is felt as true by the target buyer.
- **Importance:** Critical
- **Evidence:** Partial
- **Current basis:** The AI transformation frameworks research validates that every major framework says "upskill" but none provides the means. The observation that "you cannot envision what agents do for your organization if you cannot think agents" is logically coherent and contrasts well with the consultancy model. McKinsey/BCG/Deloitte all publish frameworks that are strategic, not experiential. But this narrative has not been tested with actual buyers. It could be perceived as anti-consultancy posturing rather than a genuine insight. Executives who have hired McKinsey may not agree that their strategy is "theatre."
- **What would disprove it:** If buyers say "we already have a clear vision for AI agents from our strategy work -- what we need is execution help, not competence building." If the buyer's problem is "we know what to do but lack capacity" rather than "we do not know what is possible."
- **Test idea:** Present two framings to interview subjects: (A) "You need a strategy for AI agents, then train people to execute it" vs. (B) "You need your people to build agents first, then the strategy becomes obvious." Ask which feels closer to their reality. If most choose A, our core narrative is wrong.

---

**D6. "Week 2, your people build a working agent with your security policies" is credible**

- **Assumption:** The promise that non-coders can build working agents with their org's real policies within 2 weeks of training is believable and deliverable.
- **Importance:** High
- **Evidence:** Partial
- **Current basis:** No-code platforms (n8n, Zapier, Make) genuinely enable non-coders to build agent-like automations (competitors-direct Round 3, e-learning Round 3). Udemy courses demonstrate that learners without coding experience can build n8n-based agents. But "building an agent in a course exercise" and "building a working agent with your company's actual security policies and compliance requirements" are very different claims. The "plug point" model (org inserts their policies, data classification, compliance requirements) has not been tested. Integrating real org-specific context into a 4-6 week cohort program is an instructional design assumption, not a validated capability.
- **What would disprove it:** If pilot cohort participants cannot get from "first exercise" to "agent that uses our real policies" within the program timeframe. If the org-specific integration (plug points) takes too long or requires technical skills the participants do not have.
- **Test idea:** Run a micro-pilot: take 3-5 non-technical volunteers from a willing organization, give them the first two weeks of curriculum, and see if they produce a working agent that incorporates at least one real organizational policy. Document what works, what breaks, and how long it actually takes.

---

**D7. Vendor-agnostic positioning is a buying criterion, not just an analyst distinction**

- **Assumption:** The buyer actively values vendor-agnostic agent education over vendor-specific training. "We want our people to understand agent patterns that transfer across any platform" is something the buyer says and pays for, not just something the analyst identifies as a structural gap.
- **Importance:** High
- **Evidence:** Weak
- **Current basis:** The research identifies vendor-agnosticism as a structural advantage (competitors-direct Round 2: "for organizations making strategic decisions about AI adoption, vendor lock-in is the problem"). But this is the analyst's framing, not the buyer's. Most organizations are already committed to a primary vendor stack (Microsoft, Google, Salesforce). The buyer might prefer training on "the platform we actually use" over training on "principles that apply anywhere." Vendor-agnostic may feel abstract compared to "build a Copilot agent this week."
- **What would disprove it:** If buyers say "we are a Microsoft shop, so we want Microsoft-specific training" or "we do not care about transferability -- we just need our people productive on our existing tools." If vendor-specific training is perceived as more practical.
- **Test idea:** In discovery interviews, ask: "If you were investing in AI agent training, would you prefer training specific to your platform (e.g., Copilot Studio) or training on agent principles that work across any platform?" Track how many choose vendor-specific vs. vendor-agnostic. If most choose vendor-specific, the positioning needs rethinking.

---

### Theme 3: Network Model Is Attractive

**D8. Companies will join and pay for a peer network around AI agent deployment**

- **Assumption:** After completing training, organizations will pay annual membership fees to remain in a cross-company network that shares agentic deployment practices. The network itself -- not just the training -- is perceived as valuable enough to sustain ongoing payments.
- **Importance:** Critical
- **Evidence:** Weak
- **Current basis:** Vistage is a $400M+ business based on peer CEO networks (practice-curators Round 1). APQC charges $15-100K for benchmarking memberships. IHI collaboratives charge participation fees. These prove the network model can work in general. But none of these exist in the AI agent space. The ai-network-curators research confirms nobody has built this for AI agents. We have no evidence that companies would pay for an AI agent peer network specifically. The analogy to IHI/Vistage/APQC is reasoning by analogy, not evidence of demand.
- **What would disprove it:** If training completers say "the training was great, but I do not need an ongoing network -- I can figure out agent deployment on my own." If companies are unwilling to share deployment practices with non-competitors. If the network feels like a "nice to have" rather than essential.
- **Test idea:** At the end of pilot cohorts, offer a 6-month free trial of the network (curated practices, peer connections, quarterly synthesis). Measure engagement: do they show up to peer calls? Do they share practices? Do they access the curated library? After 6 months, ask if they would pay. Engagement data is a leading indicator of willingness to pay.

---

**D9. The network is valued as a cross-industry peer group, not a within-industry group**

- **Assumption:** A bank, a logistics company, and a healthcare org all deploying AI agents will find value in sharing practices with each other. The business model assumes companies from different industries face "the same novel problem (agent adoption) and learn faster together." Cross-industry composition is a feature, not a limitation.
- **Importance:** High
- **Evidence:** Weak
- **Current basis:** The canvas states "none of them compete with each other -- they all face the same novel problem." IHI collaboratives work cross-hospital (same industry). APQC benchmarking is cross-industry. Vistage puts non-competing CEOs together. But we do not know whether a bank's agent security challenge and a logistics company's agent access challenge are similar enough to generate useful peer learning. The assumption that "agent adoption" is a shared enough problem to transcend industry differences is untested.
- **What would disprove it:** If peer sessions produce feedback like "their challenges are too different from ours" or "we need to talk to other financial services companies, not logistics companies." If the most valuable exchanges happen within-industry, not cross-industry.
- **Test idea:** In pilot cohorts, deliberately mix industries and track which peer interactions participants rate as most valuable. If within-industry connections consistently outperform cross-industry ones, the network composition model needs adjusting.

---

### Theme 4: Peer Sharing Actually Happens

**D10. Organizations will share their agentic deployment practices with peers**

- **Assumption:** Companies that deploy agents will share what worked, what failed, and how they adapted -- at sufficient depth and honesty to generate a valuable curated practice library. The IHI "monthly data posting" model transfers to the commercial AI agent context.
- **Importance:** Critical
- **Evidence:** None
- **Current basis:** IHI collaboratives require monthly data sharing and it works in healthcare, where a patient safety mission overrides competitive instincts (practice-curators Round 2). APQC gets members to share benchmarking data through anonymization. But AI agent deployment is commercially sensitive. A company's agent security architecture, its failure modes, its cost savings -- these are competitive intelligence. The practice-curators research itself flags this: "Will companies share their agentic practices with peers who might be competitors?" The canvas acknowledges the need for anonymization but does not test whether companies will share even anonymized practices.
- **What would disprove it:** If pilot participants are willing to discuss general challenges but unwilling to share specific practices, metrics, or failure details. If legal/compliance teams block participation in shared practice libraries. If the depth of sharing is too shallow to curate meaningful practices.
- **Test idea:** In pilot cohorts, ask participants to share one specific deployment practice (anonymized) and one specific failure/challenge. Measure: (a) willingness -- do they share at all? (b) depth -- is the sharing specific enough to be useful? (c) barriers -- do they cite legal, competitive, or organizational reasons for not sharing?

---

**D11. Champions will co-facilitate and extend the training inside their organizations**

- **Assumption:** After completing the cohort program and champion development, internal champions will actively run adapted versions of the training within their organizations. The Phase 2 and Phase 3 channel model depends on champions as the delivery mechanism.
- **Importance:** High
- **Evidence:** Weak
- **Current basis:** The practice-curators research describes IHI's model where teams include "a senior sponsor, a domain expert, and an improvement lead" who maintain improvement after the collaborative ends. Toyota Kata has a coaching model where trained practitioners coach others. But IHI itself acknowledges that "no sustainability plan after the collaborative ends" is a known failure mode. The assumption that champions will voluntarily become internal trainers and facilitators requires that (a) they have organizational permission, (b) they have time, (c) they have the facilitation skills, and (d) their organizations value this enough to make it part of their role.
- **What would disprove it:** If champions complete the program but revert to their day jobs and do not run internal sessions. If organizations do not give champions time to facilitate. If champion attrition after the program is high.
- **Test idea:** In the first cohort, identify 2-3 participants who show facilitation aptitude. Offer champion development. Track whether they actually run even one internal session within 3 months of completing the program. If none do, the champion model is aspirational, not operational.

---

### Theme 5: Retention / Ongoing Value

**D12. Curated practice packs are valuable enough to sustain annual membership**

- **Assumption:** Anonymized, curated practice packs ("how 30 companies handled agent security") are the Phase 2 product that keeps organizations paying annual network membership. This curated intelligence is seen as more trustworthy and useful than consultancy advice, vendor documentation, or independent research.
- **Importance:** Critical
- **Evidence:** None
- **Current basis:** APQC charges $15-100K for benchmarking access. CNCF generates tens of millions from the ecosystem it curates. The canvas argues that "peer-validated, battle-tested patterns" from the network are more trusted than consultancy frameworks. But we have zero evidence that curated AI agent practice packs exist, are producible, or would be valued. The network does not yet exist, so the practices do not yet exist. This is a multi-step assumption: (1) companies join, (2) they deploy agents, (3) they share what worked, (4) we curate it well, (5) others find the curated output valuable enough to pay for. Each step has its own risk.
- **What would disprove it:** If the curated output feels too generic ("obvious best practices anyone could find"), too thin (not enough depth from the network), or too stale (the agent landscape moves faster than quarterly curation cycles). If companies prefer real-time peer conversations over packaged practice packs.
- **Test idea:** Before the network exists, create a "proto-practice pack" from publicly available case studies, conference talks, and published deployment stories. Show it to potential buyers and ask: "If this came from 20 companies in a peer network you were part of, would this be worth paying for?" Gauge reaction to the format and depth.

---

**D13. The flywheel actually spins -- network effects compound**

- **Assumption:** Each new organization that joins makes the network more valuable for every existing member (classic network economics). More orgs = richer adaptation data = better curated practices = more attractive network. The flywheel creates a self-reinforcing growth loop.
- **Importance:** Critical
- **Evidence:** None
- **Current basis:** Network economics are well-documented in platform businesses (Uber, Airbnb, LinkedIn). CNCF and IHI demonstrate network effects in practice communities. But network effects are not guaranteed -- they require critical mass, active participation, and curated quality. Many community-based businesses have failed because the flywheel never started spinning (the network was too small to generate useful practices, so new members did not see value, so the network stayed small). The cold-start problem is real and acknowledged in the practice-curators research but not solved.
- **What would disprove it:** If early members do not perceive value from the (small) network and churn. If the quality of curated practices does not improve noticeably as the network grows from 5 to 15 to 30 orgs. If growth stalls after initial cohorts.
- **Test idea:** Track a "network value score" in pilot cohorts: ask members quarterly to rate (1-10) how valuable the network is to them. If the score does not increase as more orgs join, the flywheel is not spinning. Also track the ratio of active contributors to passive consumers -- if it skews heavily passive, the contribution engine is broken.

---

### Theme 6: Timing and Urgency

**D14. The buyer feels urgency to build agent capability NOW**

- **Assumption:** The target buyer feels that 2026 is the moment to invest in building internal agentic AI capability -- that waiting is costly and acting now creates advantage. Without this urgency, the buyer will defer ("interesting, but not this quarter").
- **Importance:** High
- **Evidence:** Partial
- **Current basis:** Gartner predicts 40% of enterprise apps will have agents by end of 2026 (up from <5% in 2025). McKinsey, Deloitte, IBM, MIT Sloan, and Google have all published agentic transformation frameworks in the last 6 months (ai-transformation-frameworks). The "agentic enterprise" concept is entering executive consciousness. But executive awareness of a trend does not equal budget urgency. Many executives have "AI strategy" on their list without a specific deadline or action attached. The gap between "we should do something about AI agents" and "let's invest EUR 50K in training this quarter" is large.
- **What would disprove it:** If discovery interviews reveal that buyers say "we're watching this space but not ready to invest yet" or "we'll revisit this in 2027." If the urgency is felt only by tech leaders, not by the business leaders who control L&D budget.
- **Test idea:** In discovery interviews, ask: "On a scale of 1-10, how urgent is building AI agent capability in your organization right now?" Follow up: "What would make it more urgent?" If average urgency is below 6, timing may be too early for the Finnish/Nordic mid-market.

---

**D15. The buyer's problem is "we lack agent competence" -- not "we lack agent strategy" or "we lack agent tools"**

- **Assumption:** The primary blocker for the target buyer is that their people do not understand agents well enough to deploy them effectively. The problem is competence, not strategy (they have McKinsey for that) and not tools (they have Microsoft/Salesforce for that).
- **Importance:** Critical
- **Evidence:** Partial
- **Current basis:** 46% of organizations cite integration with existing systems as their primary deployment challenge; 75% prioritize security, compliance, and auditability (ai-network-curators). These are organizational and process challenges, not tool problems. The transformation frameworks all identify workforce capability as a gap. But these are analyst observations about the market, not buyer-articulated needs. The buyer might frame their problem differently -- "we need a strategy," "we need a technology partner," "we need to hire AI talent" -- and not recognize "we lack agent competence" as the actual blocker.
- **What would disprove it:** If buyers describe their primary AI challenge as "finding the right vendor/platform," "hiring technical talent," or "getting board/budget approval" rather than "our people don't understand agents well enough."
- **Test idea:** Open-ended question in discovery interviews: "What is the single biggest thing holding your organization back from deploying AI agents effectively?" Do not prompt with options. Code the responses. If "our people lack understanding/competence" is not in the top 3 responses, the problem framing needs adjustment.

---

## Viability Assumptions (Brief)

**V1. Organizations will pay EUR 5-15K per cohort seat (per-seat, cohort-based)**
- **Importance:** Critical
- **Evidence:** Partial -- UT Austin charges $2-5K+ for a similar audience; DASA bootcamp is ~EUR 2K; Aalto EE charges similar ranges for exec ed. Price range is market-consistent but untested for this specific product.

**V2. Annual network membership fee (per-org) generates enough recurring revenue to sustain curation and community operations**
- **Importance:** Critical
- **Evidence:** Weak -- proven in other domains (APQC at $15-100K, Vistage at $15-25K), but unproven for AI agent networks. Depends on D8, D10, and D12 being true first.

**V3. Revenue can cover the cost of facilitators who can adapt (not script-readers) plus continuous frontier research**
- **Importance:** High
- **Evidence:** Weak -- the cost side is estimable (facilitator rates, research time), but revenue projections depend on volume assumptions that are themselves unvalidated.

**V4. The business reaches break-even within a reasonable timeframe with available capital**
- **Importance:** High
- **Evidence:** Partial -- calculable from cost and pricing assumptions, but sensitive to volume (number of cohorts, network membership uptake) and timeline (how fast the flywheel starts).

**V5. Champions delivering internally at zero cost to us is a real operating model, not theoretical**
- **Importance:** High
- **Evidence:** Weak -- depends on D11. If champions do not actually facilitate, the cost structure does not have the network economics described.

---

## Feasibility Assumptions (Brief)

**F1. We can build curriculum that genuinely makes agent-building accessible to non-coders without dumbing it down**
- **Importance:** Critical
- **Evidence:** Partial -- no-code platforms (n8n, Zapier) demonstrate technical feasibility; Udemy courses show non-coders can build agents. But "course exercise" and "real org-specific agent with real policies" are different. The instructional design challenge is acknowledged throughout the research as "the hardest problem."

**F2. The "framework with plug points" model works in practice -- orgs can meaningfully insert their own context within a cohort timeframe**
- **Importance:** High
- **Evidence:** None -- this is a curriculum design hypothesis. No one has tested whether org-specific policy integration can happen within a 4-6 week cohort.

**F3. We can recruit and develop facilitators who can work with org-specific context on the fly**
- **Importance:** High
- **Evidence:** Weak -- this requires a rare combination of agentic AI knowledge, facilitation skill, and business context fluency. The talent pool is unknown.

**F4. The frontier research + network curation synthesis can be maintained at quality and pace**
- **Importance:** High
- **Evidence:** Weak -- the canvas identifies this as "the critical activity." It requires tracking Agentics Foundation, protocols, tooling, academic work continuously. This is labor-intensive and the team is small.

**F5. Practice packs can be curated, anonymized, and quality-controlled from network contributions**
- **Importance:** High
- **Evidence:** Partial -- APQC and IHI demonstrate this is possible in healthcare and benchmarking. But the editorial/curatorial process for AI agent practices does not exist yet. Quality control standards are undefined.

---

## The Leap-of-Faith Map

```
                        EVIDENCE
              None         Weak         Partial       Strong
          +------------+------------+------------+------------+
          |            |            |            |            |
          | D10        | D1         | D5         |            |
 Critical | D12        | D8         | D15        |            |
          | D13        |            |            |            |
          |            |            |            |            |
          +------------+------------+------------+------------+
          |            |            |            |            |
          | D2         | D3         | D14        |            |
 High     |            | D7         | D4         |            |
          |            | D9         | D6         |            |
          |            | D11        |            |            |
          +------------+------------+------------+------------+
          |            |            |            |            |
 Medium   |            |            |            |            |
          |            |            |            |            |
          +------------+------------+------------+------------+

 LEAP OF FAITH = Top-left quadrant (Critical importance + None/Weak evidence)
```

---

## Leap-of-Faith Assumptions: Priority Testing Order

These are the assumptions that would break the model if wrong and have the least evidence. Test these first in validation interviews and early pilots.

### Priority 1: D10 -- Organizations will share their agentic deployment practices with peers
**Why first:** The entire Phase 2 value proposition (curated practice packs, network intelligence, the flywheel) depends on companies actually sharing. If sharing does not happen, the network has no content, the flywheel does not spin, and the model collapses to a one-time training business. This is the single riskiest assumption because it is the foundation of the compounding model -- and it has zero evidence. Healthcare mission-driven sharing (IHI) is not a valid proxy for commercially sensitive AI deployment sharing.

### Priority 2: D12 -- Curated practice packs are valuable enough to sustain membership
**Why second:** Even if companies share (D10), the curated output must be good enough to pay for year after year. This is the recurring revenue engine. Without it, Agents 102 is a one-time training product, not a network business. Testing this does not require a live network -- a proto-practice pack from public sources can gauge buyer interest in the format.

### Priority 3: D1 -- The "builder leader" persona exists in sufficient numbers
**Why third:** The entire buyer thesis rests on this persona. If most leaders prefer to outsource AI to consultancies rather than build internal capability, the market is too small. This is testable through discovery interviews immediately -- no product needed.

### Priority 4: D13 -- The flywheel actually spins
**Why fourth:** Network effects are the moat. Without them, Agents 102 is defensible only on curriculum quality (which is copyable). But the flywheel requires critical mass, active participation, and visible value increase. This is testable only after initial cohorts exist, so it is a medium-term test -- but it must be designed for from day one (build measurement into the pilot).

### Priority 5: D8 -- Companies will join and pay for a peer network
**Why fifth:** Even if the builder leader exists (D1) and values the training, the jump from "I liked the training" to "I will pay annually for a peer network" is significant. Vistage and APQC prove the model generally, but in the AI agent space specifically, this is untested. Testable via pilot cohort exit surveys and 6-month free trial engagement metrics.

### Priority 6: D15 -- The buyer's problem is competence, not strategy or tools
**Why sixth:** If the buyer frames their problem as "we need a strategy" or "we need the right platform," the Agents 102 narrative ("competence is the prerequisite for vision") does not land. This is directly testable in discovery interviews through open-ended problem articulation questions.

### Priority 7: D2 -- The buyer distinguishes Agents 102 from existing options
**Why seventh:** Even if the analytical gap is real, the buyer must perceive it. If Aalto EE or vendor training feels "good enough," demand is pre-empted. Testable by describing the Agents 102 offering and asking whether it addresses an unmet need.

### Priority 8: D5 -- "Competence is the prerequisite for vision" resonates
**Why eighth:** This is the core narrative, not just a value proposition. If it falls flat with buyers -- if they believe strategy precedes competence rather than the reverse -- the entire positioning needs reworking. Testable through the A/B framing question described above.

---

## What This Map Tells Us

The business model has strong competitive validation (the structural gap is real and well-documented across six research tracks) but weak demand validation (no buyer has confirmed any of the desirability assumptions). The research has answered "is anyone else doing this?" convincingly (no) but has not yet answered "does anyone want this?" at all.

The highest-risk cluster is around the **network/sharing/curation model** (D10, D12, D13). These assumptions are the foundation of the compounding value, the moat, and the long-term revenue -- and all three have zero direct evidence. If these fail, Agents 102 is a competent training product (which could still be a viable business) but not the network-driven flywheel described in the canvas. The team should be honest about which business they are testing: the training business or the network business. Testing the training business first and the network business second may be the pragmatic path.

The second highest-risk cluster is around the **buyer persona and problem framing** (D1, D2, D15). These determine whether there is a buyer at all. They are testable immediately through 10-15 structured discovery interviews -- before any product is built. This should be the next action.
