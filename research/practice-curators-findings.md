# Practice/Competence Curator Business Models — Findings & Orientation Log

**Track**: Practice Curators — Organizations that curate best practices from practitioner networks

---

## Round 1 — 2026-02-14

### Observe

Searched: "organizations that curate best practices from member networks practitioner communities business model" and "industry consortium curate member practices peer learning network revenue model examples." Identified 5 organizations that operate variations of the "practice curator + peer network + research" model, cataloged in practice-curators-resources.md:

1. APQC — Nonprofit benchmarking and best practice curation from member organizations
2. Gartner — Analyst-driven research informed by practitioner data, sold via subscription
3. Vistage / YPO — Peer advisory networks where the group itself is the product
4. CNCF / Linux Foundation — Open-source foundation that curates member contributions, with training as entry point
5. IHI (Institute for Healthcare Improvement) — Improvement collaboratives that generate and curate practices, with training as the funnel

### Orient

**Three distinct models emerge, not one:**

The five organizations reveal three structural patterns for practice curation:

1. **The Benchmarking Model** (APQC): Collect standardized data from members, aggregate and anonymize it, sell the benchmarks and best practice reports back. The curator is a neutral data processor. Members contribute data; they receive comparative intelligence.

2. **The Analyst Model** (Gartner): Hire analysts who study practitioners, interview leaders, and synthesize their own frameworks. Practitioners are data sources, not co-creators. The curator is an authority, not a facilitator. This is the most commercially successful model (Gartner: ~USD 6B revenue) but the least participatory.

3. **The Collaborative Model** (IHI, CNCF, and to some extent Vistage/YPO): Create structured spaces where practitioners work together on shared challenges. Practices emerge from the collaboration, and the curator packages and publishes them. Training feeds the pipeline of practitioners. This is the most participatory model and the closest match to what Agents 102 envisions.

**The Agents 102 model is a hybrid of CNCF and IHI, with Vistage's intimacy:**

- From CNCF: Training and certification as the entry point and revenue stream. A curated "landscape" that maps the ecosystem. Member companies as contributors. Governance neutrality.
- From IHI: The closed loop — training creates practitioners, practitioners join collaboratives, collaboratives generate practices, practices become training. The "bundle" or "practice pack" as a packaging format.
- From Vistage: Small-group peer dynamics. Confidential, facilitated discussions. The "network is the product" ethos.
- NOT from Gartner: Agents 102 should not position itself as an authority telling practitioners what works. It should position itself as a curator surfacing what its network discovers. The difference is subtle but structurally important — it determines whether members see themselves as contributors or consumers.

**The common revenue model is membership + training + events:**

Every organization in the catalog uses some combination of:
- Membership fees (recurring, tiered by organization size)
- Training fees (per-seat, per-course, or certification-based)
- Event/conference revenue
- Advisory/consulting services (for some)

No organization relies on a single revenue stream. The most resilient models (CNCF, IHI) have at least three streams. The most fragile (pure Vistage model) depend heavily on membership retention.

For Agents 102, this suggests a revenue architecture of:
1. Training fees (the entry point — immediate revenue)
2. Network membership (the recurring base — compounding value)
3. Practice pack licensing or events (the scale play — low marginal cost)

**How they build and maintain trust:**

Three trust mechanisms appear across all models:
1. **Neutrality** — APQC and CNCF are nonprofits. Gartner's analysts are nominally independent. IHI is mission-driven. No one trusts a curator with a product to sell.
2. **Reciprocity** — Members give data/practices and receive curated intelligence. The exchange must feel fair. APQC makes this explicit: "contribute your benchmarks, access everyone's."
3. **Quality control** — CNCF's maturity levels (sandbox/incubating/graduated). IHI's improvement science methodology. Gartner's analyst rigor. Someone has to filter signal from noise, and the curator's judgment is the product.

For Agents 102: Trust requires being perceived as a neutral curator, not a vendor. The training program builds credibility. The network membership creates reciprocity. Quality control of practice packs — deciding what gets curated and what does not — is the most important editorial function.

**What surprised us:**

- **IHI is the most complete model match, and we did not find it through web search — it came from prior knowledge.** This suggests the best analogs may not show up in searches for "practice curation business model." They show up in domain-specific searches. There may be more IHI-like models in other industries (manufacturing, education, safety) that are worth finding.

- **Vistage is a USD 400M+ business based purely on "CEOs talking to other CEOs."** No curated output. No published practices. No research. Just the network. This proves that the peer network alone has enormous value — everything Agents 102 adds (curation, research, practice packs) is upside on top of a proven base.

- **CNCF's "training as entry point" model generates tens of millions in certification revenue alone.** Kubernetes certifications (CKA, CKAD, CKS) at USD 395 each, with hundreds of thousands of exam takers. Training is not a loss leader — it is a major revenue line that also feeds the network.

- **No one combines all the elements Agents 102 envisions.** APQC curates but does not do training-as-entry-point. CNCF does training-as-entry-point but does not have the intimate peer network. Vistage has the peer network but does not curate or publish. IHI comes closest but is mission-driven nonprofit, not a commercial network. The full combination — training entry point + intimate peer network + practice curation + frontier research — does not exist as a single model.

**Current hypothesis about the Agents 102 business model:**

The model is viable because each component has been proven independently at scale. What is unproven is the combination. The risk is not "does practice curation work?" (APQC says yes) or "does a peer network have value?" (Vistage says yes) or "can training be the entry point?" (CNCF says yes). The risk is: can a single organization do all three well, at the scale needed to sustain all three?

The IHI closed loop offers the most promising architecture: training feeds practitioners into the network, the network generates practices, practices become training content. This creates a flywheel where each element strengthens the others. Without the loop, each element requires independent investment and justification. With the loop, they compound.

### Decide

**What should Round 2 explore?**

Two angles are most valuable:

1. **Deep dive on IHI's collaborative model mechanics.** How do IHI collaboratives actually work? How do they recruit organizations? What does the "change package" curation process look like in practice? How do they maintain quality? What is the failure rate of collaboratives? This is the mechanical blueprint for Agents 102's network operation.

2. **Domain-specific practice curators we may have missed.** Industries with strong "practitioner network + practice curation" models: manufacturing (lean/Toyota Production System networks), education (teaching practice networks), safety (aviation safety reporting systems), software engineering (ThoughtWorks Technology Radar as a model?). These may reveal mechanics and pitfalls not visible in the five examples found so far.

**What is still unclear:**

- How do you bootstrap the network? All five examples are decades old and established. How did they get the first 20-50 members? This is the cold-start problem and may be the hardest operational challenge.
- What is the right price point for Agents 102's membership tier? APQC charges USD 15-100K (enterprise). Vistage charges USD 15-25K (individual CEO). CNCF charges USD 5-370K (by tier). The range is enormous.
- Should the practice curation be open (published freely, revenue from membership/training) or closed (members-only access)? CNCF is open. APQC is closed. Both work. The choice shapes the growth model.
- How does Agents 102 handle competitive dynamics? If Company A and Company B are both members, will they share their agentic practices openly? APQC solves this through anonymization. Vistage solves it by putting non-competing companies in the same group. What is Agents 102's approach?

---

## Round 2 — 2026-02-14

### Observe

Searched: "IHI Breakthrough Series Collaborative model mechanics how it works operational blueprint change package curation process" and "Toyota Kata community practice network MEP Manufacturing Extension Partnership collaborative improvement model Carnegie Foundation networked improvement community education." Found detailed operational mechanics of the IHI BTS model and two additional collaborative models (Carnegie Foundation NICs, NIST MEP + Toyota Kata). Cataloged three new entries in practice-curators-resources.md:

6. IHI Breakthrough Series Collaborative — Operational deep dive into the BTS mechanics
7. Carnegie Foundation — Networked Improvement Communities (NICs) in education
8. NIST MEP + Toyota Kata — Manufacturing improvement network and meta-practice community

### Orient

**The operational blueprint for a practice collaborative is now clear.**

Across IHI BTS, Carnegie NICs, and MEP/Toyota Kata, a consistent operational pattern emerges. This is the blueprint:

**Phase 1 — Frame the problem**
- Identify a specific, measurable gap between current practice and desired state
- The gap must be concrete enough to measure progress and narrow enough to act on
- IHI: "large gap between what is known and what is done." Carnegie: "well-specified common aim." Toyota Kata: "target condition vs. current condition."
- All three agree: vague aims kill collaboratives. "Improve agentic capability" is too broad. "Reduce time-to-deploy for customer service agents from 6 months to 6 weeks" is specific enough.

**Phase 2 — Assemble the initial knowledge**
- Expert faculty (IHI) or hub leadership (Carnegie) develops an initial "change package" — a curated hypothesis about what practices will close the gap
- This is NOT the final output. It is the starting point, explicitly designed to be refined by practitioners
- IHI calls it "change package." Carnegie calls it "shared theory of improvement." Toyota Kata calls it "starter kata."
- For Agents 102: the "agentic practice pack v0.1" — assembled from expert knowledge, literature, and early adopter experience. It is the invitation, not the answer.

**Phase 3 — Recruit and convene**
- 20-100 organizations join (IHI). 10-50 organizations join (Carnegie). MEP works 1:1 but across 35,000 manufacturers.
- Each organization sends a small team (IHI: ~3 people — a senior sponsor, a domain expert, and an improvement lead)
- The senior sponsor is critical. Without organizational leadership support, teams cannot implement changes.
- Face-to-face convening matters. IHI uses three Learning Sessions over 6-15 months. Carnegie uses regular convenings. MEP goes on-site.

**Phase 4 — Test and share in structured cycles**
- Between convenings, teams test changes using PDSA (Plan-Do-Study-Act) cycles in their own organizations
- Results are shared monthly — performance data posted to shared platforms, written reports, calls, and peer-to-peer exchanges
- This is the practice generation engine. Every team is both testing and contributing.
- Transparency is enforced through monthly data posting — not optional sharing. This creates healthy peer pressure and a real-time learning dataset.

**Phase 5 — Curate and refine**
- The change package evolves during the collaborative based on what teams discover
- High-performing teams teach others at subsequent Learning Sessions
- Expert faculty/hub leadership synthesizes the emerging patterns
- By the end, the change package reflects collective intelligence — not just expert opinion
- Carnegie adds a critical nuance: test across diverse contexts (urban/rural, large/small, different industries) to understand WHERE a practice works, not just WHETHER it works

**Phase 6 — Publish and scale**
- Refined change packages are published as white papers, bundles, training content
- These feed back into the training pipeline and become the starting material for future collaboratives
- IHI's published bundles (e.g., the Central Line Bundle, the Ventilator Bundle) have been adopted by thousands of hospitals worldwide — far more than participated in the original collaborative
- This is where the flywheel closes: collaborative output becomes training input becomes new practitioners becomes new collaborative participants

**Common failure modes — synthesized across all three models:**

1. **Vague aims.** "Improve quality" or "adopt AI agents" is too broad. The aim must be specific and measurable. Without it, teams cannot tell if their changes are working.

2. **Lack of organizational sponsorship.** Teams that join without senior leadership support get stuck when changes require organizational buy-in or resources. IHI explicitly requires a senior sponsor on each team.

3. **Free-riding / no data sharing.** Teams that attend Learning Sessions but do not test changes or share data between sessions extract value without contributing. The monthly data posting requirement counteracts this, but enforcement is difficult.

4. **Weak hub leadership.** Carnegie identifies this as the single biggest risk. Without active coordination, synthesis, and network maintenance by the hub, collaboratives devolve into loose information-sharing groups. The hub must be resourced and empowered.

5. **Change package too rigid or too vague.** Too rigid: teams cannot adapt to their context and feel imposed upon. Too vague: teams do not know what to test. The sweet spot is specific enough to act on, flexible enough to adapt.

6. **No sustainability plan after the collaborative ends.** The 6-15 month collaborative generates energy and results, but without a plan for ongoing improvement, organizations revert. IHI addresses this in Learning Session 3 but acknowledges it is the weakest link.

**What this means for the Agents 102 model — refined understanding:**

Round 1 identified the IHI flywheel as the architecture. Round 2 reveals the operational mechanics inside that flywheel. Three refinements:

1. **The "agentic practice pack" must be a living document, not a finished product.** The change package is a starting hypothesis that gets refined through collaborative testing. Agents 102 should design practice packs as versioned, evolving artifacts — v0.1 from experts, v1.0 from the first collaborative cohort, v2.0 from the second.

2. **Context variation is the curation challenge.** Carnegie's insight is critical: a practice that works for a 500-person tech company may fail at a 50-person manufacturer. Agents 102 must curate practices with context tags — company size, industry, agentic maturity level, organizational culture. "This practice worked for X type of organization under Y conditions" is far more useful than "this is best practice."

3. **The meta-practice layer is as important as the specific practices.** Toyota Kata teaches people *how to improve*, not *what to do*. This is more durable than any specific practice pack. Agents 102 should teach an "agentic improvement kata" — a structured pattern for discovering, testing, and adopting agentic practices — alongside specific practice packs. The kata outlasts any individual technology or tool.

**Specific mechanics Agents 102 should adopt:**

- **From IHI BTS**: The three-Learning-Session structure over 6-12 months. The change package as starting material. Monthly data sharing as a requirement. The "all-teach, all-learn" ethos. PDSA cycles adapted as build-measure-learn sprints.
- **From Carnegie NICs**: The five essential building blocks (common aim, shared problem understanding, shared theory of improvement, improvement science discipline, hub leadership). Context variation testing. Hub leadership as a funded, resourced role.
- **From Toyota Kata / MEP**: The meta-practice approach — teach the improvement pattern, not just the practices. Coaching/facilitation as a delivery model alongside content. Distributed local presence (MEP centers) as an adoption multiplier.

### Decide

**Is another round needed?**

Not immediately for the practice collaborative model — the operational blueprint is now detailed enough to design a pilot. The mechanics are clear: topic selection, change package, Learning Sessions, Action Periods, PDSA cycles, monthly data, refined output, published practice packs.

**What is still unclear — but does not require more research rounds to resolve:**

1. **Bootstrapping.** How to recruit the first 10-20 organizations for the first collaborative. IHI had the advantage of a healthcare quality crisis and a charismatic founder (Don Berwick). Agents 102 has the AI urgency but needs a specific, compelling topic for its first collaborative. This is a strategy decision, not a research question.

2. **Pricing the collaborative.** IHI charges participation fees but also receives grants and philanthropy. Agents 102 needs a commercial model. The training program generates initial revenue; the collaborative could be a premium offering for training alumni. Pricing needs to be tested, not researched.

3. **The facilitator/coach model.** Who leads the Learning Sessions? Who coaches teams between sessions? IHI uses expert faculty. Carnegie uses hub leadership. MEP uses center staff. Agents 102 will need a facilitation cadre — either internal or trained external coaches. This is an operational design question.

4. **Competitive sensitivity.** Will companies share agentic practices with peers who might be competitors? The BTS model works in healthcare where "competitors" share because the mission (patient safety) overrides competitive instincts. AI agents are more commercially sensitive. Agents 102 may need Vistage-style non-competing cohorts or APQC-style anonymization — or both.

**Recommendation**: The practice curator research track has yielded a clear operational blueprint. The next step is not more research — it is to design the first Agents 102 collaborative using the IHI BTS structure, informed by Carnegie's context variation approach and Toyota Kata's meta-practice layer. The open questions (bootstrapping, pricing, facilitation, competitive sensitivity) are best resolved through design and piloting, not through further desk research.

---
