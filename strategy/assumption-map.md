# Assumption Map — Agents 102

Synthesized from Martin WWHTBT analysis and Bland/Strategyzer assumption mapping. Priority-ordered by criticality x uncertainty. Updated 2026-02-15 with F-Secure market evidence and Nordic large-company targeting.

**Evidence anchor:** An existing version of this training is being delivered at F-Secure. This is real market acceptance, not hypothesis.

**Where to play (current):** Large Nordic companies in two segments: (1) software companies, (2) traditional companies striving for high digitalization.

---

## Priority 1: The 80/20 curation model works — frontier research + peer premium

- **Category:** Desirability — Network value
- **Criticality:** Existential. The Step 5 (Drive into Value Chains) value (curated practices, network membership) depends on this.
- **Evidence:** Partial — frontier research curation is within our capability; peer sharing layer is untested.
- **The bet:** ~80% of curated practice value comes from frontier research — public case studies, conference talks, open-source patterns, published deployments. We curate what works and what doesn't from the best available sources. The remaining ~20% is the peer premium: specific, contextual, "how we actually handled it at our company" detail that no public source provides. This 20% is what makes the library peer-trusted, not just well-researched.
- **Reframe:** The question is NOT "will companies share structured data?" (IHI model). It's "will executives share naturally in a peer forum designed for how they already share?" Executives share in dinners, YPO-style forums, fireside chats, off-the-record conversations. Nordic high-trust business culture is a tailwind. The design challenge is the forum format, not the sharing behavior.
- **What must be true:** (a) We can curate frontier best-in-class agentic practice at quality and pace — knowing what works and what doesn't is the key skill. (b) The peer forum format triggers natural executive sharing. (c) The 20% peer layer adds enough specific, contextual value to justify membership over just reading our research. (d) Reciprocity or exclusivity incentives sustain contribution.
- **What would disprove it:** If the 80% frontier curation feels commoditized (anyone can do it). If peer forums produce only surface-level conversation, not actionable specifics. If the 20% peer premium isn't perceived as worth paying for.
- **Test:** (a) Create a proto-practice pack from public sources — test whether the 80% curation alone has value. (b) In pilot cohorts, run a peer forum session (dinner/fireside format) and observe depth of natural sharing. (c) After both, test whether the combination (curated research + peer specifics) feels distinctly more valuable than either alone.

---

## Priority 2: Curated practice packs are valuable enough to sustain annual membership

- **Category:** Desirability — Retention
- **Criticality:** Existential. This is the recurring revenue engine and the Step 5 (Drive into Value Chains) product.
- **Evidence:** None
- **The bet:** Anonymized, curated practice packs ("how 30 companies handled agent security") are seen as more trustworthy and useful than consultancy advice, vendor docs, or analyst reports. Companies pay annually for this.
- **What must be true:** (a) The curated output is specific enough to be actionable, not "obvious best practices anyone could find." (b) Curation keeps pace with the field — quarterly cycles don't feel stale. (c) Companies prefer packaged peer intelligence over real-time peer conversations.
- **What would disprove it:** If the curated output feels too generic, too thin, or too stale. If companies prefer ad-hoc peer conversations over structured packs.
- **Test:** Create a proto-practice pack from publicly available case studies and deployment stories. Show to potential buyers: "If this came from 20 companies in a peer network you were part of, would this be worth paying for?" Gauge reaction before the network exists.

---

## Priority 3: The builder-leader buyer exists in sufficient numbers in large Nordic companies

- **Category:** Desirability — Buyer
- **Criticality:** Critical. No buyer, no business.
- **Evidence:** Partial-strong — F-Secure is buying. 9 of 15 Nordic digital leaders publicly frame the challenge as competence/people across both target segments. See `research/nordic-digital-leaders-findings.md`.
- **The bet:** There is a meaningful population of CEOs/CTOs/SVPs at large Nordic software companies and digitalizing traditional companies who want to own AI agent transformation internally rather than outsource it — and who will invest budget in capability-building over advisory.
- **What must be true:** (a) The psychographic is real and recognizable, not an idealized portrait. (b) These leaders have budget authority without routing through procurement that defaults to Aalto EE or an incumbent. (c) They have already experienced and rejected transformation theatre. (d) The two segments (software cos, digitalizing traditional cos) together form a large enough addressable market.
- **F-Secure evidence:** Confirms at least one software company buyer exists. Does not yet confirm the "traditional company striving for digitalization" segment. Does not confirm volume.
- **What would disprove it:** If outreach finds most leaders prefer to hire Solita/Reaktor to "do AI for us." If the response is "we already have an AI partner" rather than "we need our own people to learn this."
- **Test:** 10-15 discovery interviews with Nordic C-suite and SVP-level leaders across both segments. Ask about their current AI approach, dependency on external partners, and appetite for building internal agentic competence. Do not pitch.

---

## Priority 4: The flywheel actually spins — network effects compound

- **Category:** Desirability — Network
- **Criticality:** Critical. Without network effects, Agents 102 is a training business, not a network business. The moat disappears.
- **Evidence:** None
- **The bet:** Each new org that joins makes the network more valuable for every existing member. More orgs = richer adaptation data = better curated practices = more attractive network.
- **What must be true:** (a) Critical mass is achievable (the first 10-20 orgs generate enough practices to attract the next wave). (b) Active participation outweighs passive consumption. (c) Curated quality visibly improves as the network grows.
- **Cold-start problem:** Step 5 value depends on training alumni contributing, but alumni will only contribute if the network already has value.
- **What would disprove it:** If early members don't perceive value from a small network and churn. If growth stalls after initial cohorts. If the contributor-to-consumer ratio skews heavily passive.
- **Test:** Track "network value score" (1-10) quarterly in pilot cohorts. Track active contributor ratio. If the score doesn't increase as more orgs join, the flywheel isn't spinning. Must be designed into the pilot from day one.

---

## Priority 5: The buyer's problem is "we lack agent competence" — not strategy or tools

- **Category:** Desirability — Problem framing
- **Criticality:** Critical. The entire positioning depends on this.
- **Evidence:** Partial-strong — 9/15 Nordic digital leaders independently frame it as competence/people in public statements. Transformation frameworks research confirms all frameworks say "upskill" without providing the means. F-Secure bought competence-building, not strategy. See `research/nordic-digital-leaders-findings.md`.
- **The bet:** The primary blocker is that people don't understand agents well enough to deploy them. Not that they lack strategy (McKinsey offers that) or tools (Microsoft offers that). Competence is the missing layer.
- **What must be true:** Buyers frame their problem as a competence gap, not a strategy gap or a hiring gap. They recognize "our people don't understand agents" as the actual blocker.
- **What would disprove it:** If buyers say "we need a strategy," "we need a technology partner," or "we need to hire AI talent" — and don't recognize competence as the bottleneck.
- **Test:** Open-ended question in discovery interviews: "What is the single biggest thing holding your organization back from deploying AI agents effectively?" Do not prompt with options. Code responses. If "our people lack understanding/competence" is not in the top 3, the problem framing needs adjustment.

---

## Priority 6: "Competence is the prerequisite for vision" resonates with buyers

- **Category:** Desirability — Narrative
- **Criticality:** Critical. This is the central intellectual claim.
- **Evidence:** Partial — logically coherent, supported by framework analysis. Untested with buyers.
- **The bet:** The agentic transformation is epistemically different from digital/agile/cloud. You can't envision it until you've built it. Buyers agree — or can be persuaded — that competence must precede strategy, reversing the normal corporate sequence.
- **What must be true:** The claim survives the objection: "I didn't need to write code to lead our digital transformation — why would I need to build an agent?" Buyers must be willing to invest in competence before they have a strategy.
- **Shelf life:** Strongest in 2026-2027 while agents are novel. May weaken as platforms become intuitive enough that vision is accessible through demos alone.
- **What would disprove it:** If buyers say "we already have a clear vision from our strategy work — we need execution help, not competence building."
- **Test:** Present two framings: (A) "You need a strategy for AI agents, then train people to execute it" vs. (B) "You need your people to build agents first, then the strategy becomes obvious." Ask which feels closer to their reality.

---

## Priority 7: Training buyers (Steps 1-4) will progress to Step 5 (Drive into Value Chains)

- **Category:** Desirability — Retention
- **Criticality:** High. Determines whether the business achieves network economics or remains training-only.
- **Evidence:** None
- **The bet:** Organizations that buy training (time-boxed, clear deliverable) will also approve ongoing annual network membership (different budget line, different value proposition, different internal justification).
- **What must be true:** The "why they stay" is compelling enough to survive annual budget reviews where the training is already done and the immediate need is met.
- **What would disprove it:** If training completers say "the training was great, but I don't need an ongoing network."
- **Test:** At end of pilot cohorts, offer 6-month free trial of the network. Measure engagement: do they show up to peer calls? Access the curated library? Share practices? After 6 months, ask if they would pay.

---

## Priority 8: The buyer distinguishes Agents 102 from existing training options

- **Category:** Desirability — Differentiation
- **Criticality:** High. If Aalto EE or vendor training feels "good enough," demand is pre-empted.
- **Evidence:** None (structural gap is analyst's observation, not buyer's perception)
- **The bet:** When a Nordic SVP of HR or CDO decides to invest in AI capability, they see Agents 102 as meaningfully different from Aalto EE's programs, vendor training, and embedded consulting from Solita/Reaktor.
- **What must be true:** The buyer perceives a gap between "understanding AI strategically" and "being able to build and deploy agents." Aalto EE and vendor training don't feel sufficient for the latter.
- **What would disprove it:** If buyers describe existing options as sufficient for building agentic capability.
- **Test:** In discovery interviews, ask what AI training they've done and what's missing. If they say "nothing is missing," the assumption fails.

---

## Priority 9: Non-technical learners can build agents meaningful enough to shift leadership's mental model

- **Category:** Feasibility — Instructional design
- **Criticality:** High. The "Week 2 demo" is the conversion moment.
- **Evidence:** Partial — no-code platforms prove technical feasibility (Udemy courses with 10K+ students). But "course exercise" and "real org-specific agent with real policies" are different things.
- **The bet:** Non-coders can build working agents with their org's real security policies, compliance requirements, and data within a 4-6 week cohort — and the result is impressive enough to shift how leadership thinks about agents.
- **What would disprove it:** If pilot participants can't get from "first exercise" to "agent that uses our real policies" within the timeframe. If the results feel like toys.
- **Test:** Micro-pilot with 3-5 non-technical volunteers from a willing org. First two weeks of curriculum. Do they produce a working agent that incorporates at least one real organizational policy?

---

## Priority 10: The buyer feels urgency to build agent capability NOW

- **Category:** Desirability — Timing
- **Criticality:** High. Without urgency, the buyer defers.
- **Evidence:** Partial-strong — F-Secure bought now. BCG: only 19% of Nordic workers use GenAI weekly vs 61% globally, EUR 60B+ GDP risk. Equinor at production scale (USD 130M saved). Novo Nordisk scaled to 20K Copilot users. Gartner predicts 40% of apps with agents by end 2026. See `research/nordic-digital-leaders-findings.md`.
- **The bet:** 2026 is the moment to invest. Waiting is costly.
- **What would disprove it:** If interviews reveal "we're watching this space but not ready to invest yet" or "we'll revisit in 2027."
- **Test:** In discovery interviews: "On a scale of 1-10, how urgent is building AI agent capability in your organization right now?" If average is below 6, timing may be too early for Nordic large companies.

---

## Priority 11: Vendor-agnostic positioning is a buying criterion, not just an analyst distinction

- **Category:** Desirability — Positioning
- **Criticality:** High
- **Evidence:** Weak — validated as structural advantage by research, but this is analyst framing, not buyer language.
- **The bet:** The buyer actively values "principles that transfer across any platform" over "training on the platform we use."
- **What would disprove it:** If buyers say "we're a Microsoft shop, we want Microsoft-specific training."
- **Test:** In interviews: "Would you prefer training specific to your platform or training on agent principles that work across any platform?" Track the split.

---

## Priority 12: Communal model attracts buyers (anti-SAFe positioning works commercially)

- **Category:** Desirability — Model preference
- **Criticality:** Medium-high
- **Evidence:** Weak — philosophically compelling, commercially untested.
- **The bet:** Buyers prefer ownership and adaptability over prescriptive frameworks with certification. The anti-SAFe positioning resonates positively.
- **Risk:** Historical evidence shows prescriptive frameworks (SAFe, ITIL, PRINCE2) capture larger markets — even when practitioners dislike them — because buyers want institutional cover. The communal model may leave the larger market to a future prescriptive competitor.
- **What would disprove it:** If buyers say "we want a clear methodology with defined roles and certifications."
- **Test:** Observe buyer language in discovery interviews. Do they ask for structure and certification, or do they value ownership and adaptability?

---

## Priority 13: Cross-industry peer network is valued (not just within-industry)

- **Category:** Desirability — Network
- **Criticality:** Medium-high
- **Evidence:** Weak — IHI works cross-hospital (same industry), APQC works cross-industry, Vistage cross-industry.
- **The bet:** A bank, a logistics company, and a software company all deploying agents find value in sharing with each other because "agent adoption" is a shared enough problem to transcend industry differences.
- **What would disprove it:** If peer sessions produce "their challenges are too different from ours."
- **Test:** In pilot cohorts, deliberately mix industries. Track which peer interactions participants rate as most valuable.

---

## Priority 14: Aalto EE will not launch "good enough" agentic AI training first

- **Category:** Desirability — Competitive timing
- **Criticality:** Medium
- **Evidence:** Partial — Aalto EE has 6 AI programs but none on agent building. They own the Finnish L&D buyer relationship.
- **The bet:** Aalto EE does not move into agentic AI training (even strategic/managerial) before Agents 102 establishes itself. Or: Aalto EE becomes a channel partner.
- **What would disprove it:** Aalto EE announces an AI agents program.
- **Test:** Monitor. Also: explore the partnership path proactively.

---

## Viability Assumptions (secondary — calculable)

| # | Assumption | Importance | Evidence |
|---|-----------|-----------|----------|
| V1 | Orgs will pay EUR 5-15K per cohort seat | Critical | Partial — UT Austin $2-5K, DASA ~EUR 2K, Aalto EE similar |
| V2 | Annual network membership generates enough recurring revenue | Critical | Weak — APQC $15-100K, Vistage $15-25K prove model generically |
| V3 | Revenue covers facilitator costs + continuous frontier research | High | Weak — estimable from cost side, depends on volume |
| V4 | Business reaches break-even within reasonable timeframe | High | Partial — calculable, sensitive to volume assumptions |
| V5 | Champions delivering internally at zero cost is operational, not theoretical | High | Weak — depends on Priority 7 and champion engagement |

## Feasibility Assumptions (secondary — testable by piloting)

| # | Assumption | Importance | Evidence |
|---|-----------|-----------|----------|
| F1 | Curriculum makes agent-building genuinely accessible to non-coders | Critical | Partial — n8n/Zapier prove technical feasibility |
| F2 | Framework + plug points works in practice within cohort timeframe | High | None — instructional design hypothesis |
| F3 | Can recruit facilitators who adapt to org context on the fly | High | Weak — rare skill combination |
| F4 | Frontier research + network curation synthesis maintainable at quality and pace | High | Weak — labor-intensive, small team |
| F5 | Practice packs can be curated, anonymized, and quality-controlled | High | Partial — APQC/IHI prove it's possible in other domains |

---

## What This Map Tells Us

The business model has strong competitive validation (the structural gap is real across six research tracks) and one confirmed market case (F-Secure). It has weak demand validation beyond that single case.

The highest-risk cluster is the **network/sharing/curation model** (Priorities 1, 2, 4, 7). If these fail, Agents 102 is a competent training product but not the network flywheel. The pragmatic path: test the training business first, the network business second.

The second highest-risk cluster is the **buyer persona and problem framing** (Priorities 3, 5, 6, 8). These determine whether there's a buyer beyond F-Secure. Testable immediately through 10-15 discovery interviews.

**Three questions to answer first:**
1. Can we curate "what works and what doesn't" well enough that the 80% frontier research + 20% peer premium creates unique value? (the curation assumption)
2. Who else is like F-Secure? (the buyer segment assumption)
3. Is the problem framed as competence, not strategy? (the positioning assumption)
