# What Would Have to Be True — Agents 102 Business Model

Roger Martin's WWHTBT framework applied to the implicit strategic choices in the Agents 102 business model. For each choice, the question is: what conditions must hold for this choice to succeed?

Focus is on **desirability** (do buyers want this?) first. Viability and feasibility are noted but secondary.

---

## Choice 1: "The builder leader is our buyer" (psychographic, not title)

The business model defines the primary buyer as a CEO/CTO/SVP of HR who is personally frustrated with transformation theatre, wants to *own* the transformation, and has previously led change personally rather than outsourcing it. This is a psychographic segment — a disposition, not a job title.

**What Would Have to Be True:**

- **About the customer:** This psychographic segment must exist in sufficient numbers within Finnish mid-market companies. The "builder leader" must be a real and recognizable type, not an idealized portrait. These leaders must be actively looking for AI agent capability-building right now (2026), not passively waiting. They must have budget authority to buy training programs without needing to route through a formal L&D procurement process that defaults to Aalto EE or an incumbent consultancy. They must perceive "my people building agents in Week 2" as more valuable than "Deloitte giving me a governance framework" — which requires them to have already experienced and rejected transformation theatre. Leaders who have not yet been burned by consultancy-led transformation may not feel the pain this positioning speaks to.

- **About the industry:** The Finnish mid-market must contain enough companies where the CEO/CTO personally drives transformation rather than delegating it. Finland's corporate culture (consensus-oriented, engineering-heavy) may actually favor this — Finnish leaders tend to be more hands-on than, say, UK or US executives — but this is assumed, not validated. The market window must also be open: if most Finnish mid-market companies have not yet started their agentic transformation, the builder leader may not yet be activated around this specific topic.

- **About our capabilities:** We must be able to identify and reach these builder leaders through channels they trust. They are not browsing Udemy or attending generic AI conferences. They are likely in peer networks (NBF attendees, YPO/EO members, Aalto EE alumni). We need a credibility signal that resonates with a leader who is skeptical of consultancies and courses alike — someone who respects practitioners, not institutions.

- **About the competition:** Builder leaders must not already have a satisfactory path to agent competence. The research shows that Aalto EE offers strategic/managerial AI programs, Solita embeds training in consulting engagements, and vendor training (Microsoft, Salesforce) is free. For our choice to work, the builder leader must see all of these as insufficient — Aalto EE too theoretical, Solita too dependent, vendor training too locked-in. If any of those options feels "good enough," the builder leader will not seek us out.

**Confidence level:** Partially validated. The psychographic is well-articulated and the competitive research confirms that no existing option targets this buyer with this combination. But no direct evidence exists that Finnish mid-market leaders self-identify this way or are actively seeking what we describe. The buyer persona is a hypothesis, not a finding.

---

## Choice 2: "We are the prerequisite, not the complement" (competence before strategy)

The business model claims that agentic transformation is fundamentally different from previous transformations (digital, agile, cloud) because you cannot envision agentic possibilities without hands-on competence. Therefore, Agents 102 is not an execution layer under McKinsey's strategy — it is the prerequisite for having a real strategy at all.

**What Would Have to Be True:**

- **About the customer:** Buyers must agree — or be persuadable — that agentic AI is epistemically different from previous transformations. That "you can't envision it until you've built it" must resonate as true. This is a strong claim. Many executives will push back: "I didn't need to write code to lead our digital transformation, why would I need to build an agent to lead our agentic transformation?" The claim must survive this objection. Buyers must also be willing to invest in competence *before* they have a strategy — which reverses the normal corporate sequence (strategy first, then capability-build). This is a behavioral change, not just a purchase decision.

- **About the industry:** The "competence before strategy" claim must hold as agentic AI matures. Right now, in early 2026, it may be true — agents are novel enough that hands-on experience genuinely shifts understanding. But if agent platforms become as intuitive as, say, building a Notion page, then the vision *could* be accessible through demos and strategy work alone — no training needed. The prerequisite claim has a shelf life. It is strongest in the current window (2026-2027) and may weaken as the technology matures and becomes self-evident.

- **About our capabilities:** We must deliver an experience that actually produces the epistemological shift we promise. Week 2 of the training must produce a genuine "aha" moment where the leader sees agents working with their company's data and their mental model changes. If the training feels like a tutorial rather than a revelation, the prerequisite claim falls flat. The instructional design must cross the chasm from "interesting exercise" to "I now see my business differently."

- **About the competition:** Consulting firms must continue to sell strategy-first, not competence-first. If McKinsey or Deloitte starts embedding hands-on agent-building workshops into their advisory engagements (a McKinsey-built "agent hackathon for the C-suite"), the prerequisite positioning collapses — they would be offering the same insight but bundled with the strategy. The research shows consultancies do not do this today and have structural disincentives (consulting economics), but the disincentive is not permanent.

**Confidence level:** Partially validated. The transformation frameworks research strongly supports the claim that current frameworks are "governance of an abstraction." But the epistemological argument — that agent competence is uniquely experiential in a way that digital/cloud/agile was not — is a philosophical position, not an empirical finding. It is the central intellectual bet of the business model.

---

## Choice 3: "The network of companies is the product" (not the training)

The business model positions the training as merely the "forum" — the entry point into a peer network of companies deploying agents. The real product is the network itself: peer trust, shared practices, curated intelligence from collective experience. This is the YPO model, not the Udemy model.

**What Would Have to Be True:**

- **About the customer:** Companies must *want* to share their agentic deployment experiences with other companies. This is the most critical desirability assumption in the entire model. It requires: (a) companies must believe they gain more from sharing than they lose in competitive advantage; (b) the people in those companies must be willing to invest time in contributing to the network, not just consuming from it; (c) the network must be perceived as a peer community, not a data extraction scheme. The IHI and CNCF models show this works in healthcare and open-source infrastructure — domains where sharing is culturally normalized. It is unproven in AI agent deployment, where companies may view their agentic capabilities as competitive differentiators worth protecting.

- **About the customer (continued):** The buyer must also value the *network* enough to pay for ongoing membership after the training ends. The training is a time-boxed purchase with a clear deliverable. The network is an ongoing relationship whose value is harder to measure. Buyers must believe "access to how 30 companies handled agent security" is worth an annual fee — and must trust that the curated intelligence is genuinely better than what they could get from a consulting engagement or analyst report.

- **About the industry:** Agent deployment must remain complex and varied enough that peer learning has durable value. If agent deployment becomes a solved problem — commoditized by vendor platforms — then there is nothing novel to share. The network's value depends on the field being hard enough that companies learn faster together than alone. The research suggests this is true in 2026 (fewer than 1 in 4 organizations have scaled agents to production), but the duration of this complexity is unknown.

- **About our capabilities:** We must be excellent curators. The network produces raw material (experiences, adaptations, war stories). We must distill this into structured, trustworthy, actionable practice packs. This is an editorial and research capability — closer to what Gartner analysts do than what trainers do. We must also facilitate the network dynamics: matching companies with similar challenges, preventing free-riding, maintaining trust, managing competitive sensitivities. These are community management skills, not training skills. The team must be good at a fundamentally different thing than delivering training.

- **About the competition:** No one else must build a credible peer network for agentic deployment first. The ai-network-curators research confirms the space is empty today. But first-mover advantage in networks is real — the first credible network captures early adopters, who generate the practices, which attract the next wave. If a well-resourced player (e.g., a Salesforce Trailblazer community for Agentforce, or a Gartner peer forum for AI leaders) launches a competing network with better distribution, the window closes.

**Confidence level:** The gap is validated with high confidence (8/10 per the research). The desirability — whether companies will participate actively — is assumed. No evidence exists that Finnish or Nordic companies want to share agentic practices in a structured peer network. The IHI/CNCF analogs prove the model works in other domains, but the transfer to this domain is a leap of faith.

---

## Choice 4: "Framework + plug points" (orgs integrate their own context)

The curriculum is not a fixed script. It is a framework with explicit plug points where each organization inserts their own security policies, compliance requirements, tools, processes, and KPIs. The training output is not a certificate — it is working agents built on the org's actual context.

**What Would Have to Be True:**

- **About the customer:** Organizations must be willing to bring their real context — real policies, real compliance requirements, real processes — into a training environment. This requires trust (the training provider sees sensitive internal information) and organizational coordination (someone must prepare and share these artifacts). If the buyer treats the training as a generic upskilling exercise ("just teach our people about agents"), the plug-point model fails. The buyer must understand and value the difference between generic training and context-integrated training — and be willing to do the extra work to make it happen.

- **About the customer (continued):** The end learners — non-technical business professionals — must be able to meaningfully integrate their org's context without extensive support. A product manager must be able to take the "agent security principles" module and apply it to their company's actual security policies. If this integration step is too difficult for non-technical learners, the plug-point model becomes consulting (we do the integration for them) rather than training (they do it themselves with our framework).

- **About our capabilities:** The framework must be robust enough to work across diverse organizational contexts. A framework that works for a 500-person tech company must also work for a 5,000-person bank. The plug points must be well-designed — specific enough to be actionable, generic enough to accommodate vastly different contexts. This is an instructional design challenge that, per the competitive research, no one has solved. The fact that no competitor offers this is evidence of both an opportunity and a difficulty.

- **About the competition:** Vendor training must remain context-free (teaching their product generically). If Microsoft Copilot Studio training starts including "bring your own policies" customization, the plug-point differentiator weakens. Currently, vendors have no incentive to do this — their scale depends on standardized training — but enterprise-tier vendor engagements might offer this as a premium service.

**Confidence level:** Partially validated. The concept is well-developed and the IHI "change package" model provides a proven analog. But the specific mechanics — which plug points, how much integration support is needed, whether non-technical learners can do it themselves — are untested. This is an instructional design hypothesis that can only be validated through piloting.

---

## Choice 5: "We are curator + researcher, not the authority"

The business model explicitly rejects the position of "authority on how to deploy agents." The authority resides in the collective experience of the network. Agents 102's role is to curate that intelligence, synthesize it with frontier research, and make it accessible. "Not our opinion — their peers' battle-tested results."

**What Would Have to Be True:**

- **About the customer:** Buyers must trust curated peer intelligence more than expert authority. This is a specific claim about trust hierarchies: "a bank trusts a logistics company that already deployed agents in a regulated environment more than it trusts any trainer or consultant." This may be true for some buyers (the builder leader who is skeptical of consultancies), but many enterprise buyers actively prefer the authority model — they want someone to tell them what to do, backed by a brand (McKinsey, Gartner) that provides institutional cover. The "curator, not authority" positioning may repel buyers who want definitive answers. "Here's how 12 companies handled it, each differently" is less reassuring than "here's the right answer, per McKinsey."

- **About the customer (continued):** Buyers must value peer validation over analyst or consultant validation. This is culturally variable. In Nordic countries, with flat organizational cultures and high-trust business environments, peer validation may be more natural than in hierarchical business cultures. This could be a tailwind in Finland specifically. But it needs validation.

- **About our capabilities:** Curating without being authoritative is a difficult editorial posture. It requires resisting the temptation to prescribe (which is what clients ask for), maintaining neutrality across vendors and approaches (which requires disciplined editorial judgment), and producing curated outputs that are genuinely useful without being prescriptive. This is a specific skill set — more journalistic/editorial than consultative — and it is unclear whether the founding team has it.

- **About the competition:** Gartner and Forrester must continue to be perceived as top-down analysts rather than peer facilitators. If Gartner launches a collaborative peer forum for agentic AI (rather than their traditional analyst model), they combine brand credibility with the peer model. The research suggests Gartner is structurally top-down, but they do operate Gartner Peer Insights and Gartner Forums, which are peer-adjacent.

**Confidence level:** Assumed. The positioning is philosophically coherent and consistent with the IHI/Vistage models. But no evidence exists that the target buyers in Finnish mid-market prefer the curator model over the authority model for AI deployment guidance. This is a cultural and psychological bet.

---

## Choice 6: "Phase 1 Train, Phase 2 Curated practices"

The business model has an explicit two-phase structure. Phase 1 (training) is the entry point and immediate revenue generator. Phase 2 (curated practices / network membership) is the long-term value and the real product. The claim is that training is how orgs enter, and curated practices are why they stay.

**What Would Have to Be True:**

- **About the customer:** Organizations that buy training (Phase 1) must convert to network members (Phase 2). This is a retention and upgrade assumption. The buyer who approved a time-boxed training cohort must also approve ongoing annual network membership — a different budget line, a different value proposition, and a different internal justification. Many organizations buy training as a one-time event and do not convert to ongoing subscriptions. The "why they stay" must be compelling enough to survive annual budget reviews where the training is already done and the immediate need is met.

- **About the customer (continued):** There must be enough Phase 1 participants contributing back to the network to make Phase 2 curated practices genuinely valuable. If early cohorts deploy agents but do not share their experiences, the curated practices library is thin and the Phase 2 value proposition is empty. This is a cold-start problem: Phase 2 value depends on Phase 1 alumni contributing, but Phase 1 alumni will only contribute if the network already has value.

- **About our capabilities:** We must be able to build Phase 2 before Phase 2 pays for itself. The curated practices library, the community operations, the frontier research synthesis — all of this requires investment before the network membership revenue materializes. The business must survive on Phase 1 training revenue long enough for Phase 2 to become self-sustaining. The financial bridge between Phase 1 and Phase 2 is a viability question, not a desirability question — but it is critical.

- **About the industry:** The agentic AI field must remain dynamic enough that curated practices have ongoing value. If best practices crystallize and become common knowledge (published in books, taught in MBA programs, embedded in vendor documentation), the curated practices lose their premium. The network's value depends on the field being in a state of active discovery — which is true now but may not be in 3-5 years.

**Confidence level:** Partially validated. The IHI closed-loop model (training feeds practitioners, practitioners generate practices, practices become training) is proven. The CNCF model shows training can generate substantial revenue while feeding a network. But the specific Phase 1-to-Phase 2 conversion in a commercial AI training context is untested. The cold-start problem for Phase 2 is real and unaddressed in the business model.

---

## Choice 7: "Communal, not prescriptive" (anti-SAFe)

The business model explicitly positions against SAFe's model: "SAFe got the action right but the rigidity wrong." Agents 102 does not prescribe how organizations should deploy agents. It teaches meta-capabilities and lets organizations build with their own structure. No certification hierarchy. No licensed frameworks. Champions adapt, not comply.

**What Would Have to Be True:**

- **About the customer:** Buyers must prefer ownership over prescriptiveness. They must want to figure things out themselves (with peer support) rather than be told what to do. This is the builder-leader psychographic again — it assumes buyers who value autonomy and are confident enough to adapt rather than follow. But many enterprise buyers want prescriptive frameworks *because* they reduce risk and provide institutional cover. "We followed the SAFe framework" is a defensible position in a board meeting. "We adapted what we learned from a peer network" is a harder sell internally. The communal model requires buyers who have the organizational confidence to own their approach.

- **About the customer (continued):** The "anti-SAFe" positioning must resonate positively, not just negatively. Many potential buyers may not have experienced SAFe's rigidity firsthand and will not recognize the critique. They may just hear "unstructured" where we mean "adaptable." The positioning needs to appeal to people who have been burned by prescriptive frameworks — a subset of the market, not the whole market.

- **About our capabilities:** Creating a communal, non-prescriptive program is harder to deliver consistently than a prescriptive one. Every facilitator must be able to work with diverse organizational contexts on the fly, without falling back on a script. The quality variance across facilitators is likely higher than in a prescriptive model. This creates a scaling challenge: where does Agents 102 find facilitators who can adapt in real-time to any org's context?

- **About the competition:** Prescriptive models must remain unpopular or discredited in the AI agent space. If someone creates a "SAFe for Agents" — a comprehensive, prescriptive framework for organizational agent adoption with certification, roles, and rituals — it would attract the large segment of buyers who want structure and institutional cover. The anti-SAFe positioning leaves this entire segment to a potential prescriptive competitor.

**Confidence level:** Assumed. The anti-SAFe positioning is philosophically compelling and well-argued in the business model. But it has not been tested with buyers. The risk is significant: the market for prescriptive frameworks is historically much larger than the market for communal ones. SAFe itself is proof — despite widespread criticism from practitioners, it generates hundreds of millions in annual revenue because buyers want the certainty.

---

## Choice 8: "Finland first, mid-market"

The business model targets Finnish mid-market companies as the initial beachhead. The research maps Helsinki consultancies, Finnish executive education providers, and Finnish corporate training buyers.

**What Would Have to Be True:**

- **About the customer:** The Finnish mid-market must be large enough and active enough in AI agent adoption to sustain the business through Phase 1. Finland's mid-market (roughly 500-5,000 employee companies) is small in absolute terms — perhaps 200-500 companies in the relevant size range. Of those, the subset that (a) is actively pursuing agentic transformation, (b) has a builder-leader buyer, and (c) would choose Agents 102 over Aalto EE/Solita/vendor training may be quite small. The initial addressable market could be 20-50 companies. Is that enough for Phase 1?

- **About the customer (continued):** Finnish companies must be willing to pay meaningful prices for training that is not from an established institutional brand (Aalto EE, a university, a global consulting firm). A new entrant without institutional backing faces a credibility gap in a market where the default is to buy from known providers. The Helsinki consultancies research shows that Aalto EE "owns the relationship" — getting Finnish enterprise L&D buyers to try an alternative requires overcoming institutional inertia.

- **About the industry:** Finland must not be too early in agentic AI adoption for the training to have demand. If Finnish mid-market companies are still in "wait and see" mode on agents, there is no urgency behind the purchase. Conversely, if they are late adopters, the timing may work — the urgency is building as they watch Nordic peers move ahead. The research does not contain direct evidence on where Finnish mid-market companies are in their agentic adoption journey.

- **About our capabilities:** We must have or build credibility in the Finnish market. The founding team's existing network, reputation, and relationships in Finland determine how quickly the first 3-5 design partner organizations can be recruited. Without a warm network, cold outreach to Finnish enterprises for a new, unproven training program is extremely difficult.

- **About the competition:** Aalto EE must not launch an "AI Agents for Leaders" program before Agents 102 establishes itself. The Helsinki consultancies research flags this as a real risk: Aalto EE already has 6 AI programs and the buyer relationship. If they move into agentic AI training (even with a strategic/managerial framing, not hands-on), they could absorb demand before Agents 102 reaches the market. Alternatively, Aalto EE could be the channel partner — but that is a choice, not a given.

**Confidence level:** Partially validated. The Finnish competitive landscape is well-mapped and the blue ocean is confirmed (no one in Finland does this). But the market size, buyer readiness, and buyer willingness to try a new entrant are unknown. Finland-first is a sensible beachhead strategy, but the market may be too small for the model's economics to work at the national level.

---

## Choice 9: "Vendor-agnostic, principle-first"

The training teaches agent patterns (reflection, tool use, planning, multi-agent) that transfer across any platform. Hands-on exercises use no-code platforms (n8n, Zapier, Copilot Studio) but the org chooses their platform. The value is in transferable thinking, not platform skills.

**What Would Have to Be True:**

- **About the customer:** Buyers must value transferable principles over immediate platform proficiency. When a buyer approves training budget, they often want measurable, tool-specific outcomes: "After this training, our team can build X in Copilot Studio." Vendor-agnostic training produces a less tangible outcome: "After this training, our team can think in agent patterns and apply them to any platform." The buyer must believe the latter is more valuable than the former — or at least believe both matter and that vendor training covers the platform-specific skills.

- **About the customer (continued):** End learners must experience the vendor-agnostic approach as liberating, not confusing. Learning principles without a fixed platform means learners must navigate ambiguity: "Which platform should I use? The training didn't tell me." If learners leave the training without a clear tool preference and a clear next step, the experience may feel incomplete. The plug-point model (org brings their preferred tools) partially addresses this, but only if the org has already chosen a platform.

- **About the industry:** The agent platform landscape must remain fragmented enough that vendor-agnostic thinking has value. If one platform achieves dominance (the way Salesforce dominated CRM), then vendor-agnostic training becomes less valuable — everyone just needs to learn the dominant platform. Current fragmentation (Copilot Studio, Agentforce, Vertex AI, n8n, Make, CrewAI, LangGraph) strongly favors vendor-agnostic positioning. But platform consolidation could erode this advantage.

- **About our capabilities:** We must keep the framework current across multiple platforms. Every hands-on exercise must work on the learner's chosen platform — which means maintaining exercises for n8n, Zapier, Copilot Studio, and potentially others. This is a maintenance burden that scales with the number of supported platforms. The alternative (picking one platform as the "teaching medium," like the business school uses Excel) reduces the burden but weakens the vendor-agnostic claim.

- **About the competition:** Free vendor training must continue to be platform-locked and principle-free. If Microsoft starts teaching transferable agent patterns in its Copilot Studio training (rather than just product operations), the vendor-agnostic differentiator weakens. The research suggests vendors have structural disincentives to do this — but competitive pressure could change that.

**Confidence level:** Validated by competitive research. The competitive analysis confirms that vendor training is structurally locked-in and principle-free, and that no independent program successfully combines vendor-agnostic principles with hands-on building. The gap is real. The desirability question — whether buyers prefer principles over platform skills — is partially validated by the fact that the builder-leader buyer would naturally value transferable capability over vendor lock-in.

---

## Choice 10: "Non-coders building agents" (the instructional design bet)

The end learner is explicitly a non-technical business professional: product managers, analysts, team leads, operations staff. They build working agents without writing code, using no-code platforms as the hands-on medium.

**What Would Have to Be True:**

- **About the customer:** Non-technical business professionals must actually want to build agents, not just understand them. There is a difference between "I want to know what AI agents are" (satisfied by a lecture) and "I want to build one myself" (requires hands-on effort and risk tolerance). The end learner must be willing to invest the effort and tolerate the frustration of building something new with unfamiliar tools. If most learners want comprehension rather than construction, the hands-on model will feel like unnecessary difficulty.

- **About the customer (continued):** The learners must be capable of building meaningful agents using no-code platforms without extensive support. The competitive research notes: "This phrase [non-coders building agents] appears in marketing copy, but when you look at the actual curricula, the programs either (a) teach coding under a friendlier label, or (b) stay at the strategy level and never actually have participants build." The gap may exist because it is genuinely hard, not because no one has thought of it. If non-technical learners cannot build agents that are useful enough to demonstrate the value to leadership, the Week 2 demo moment (the core conversion event) fails.

- **About our capabilities:** The instructional design must solve the accessibility-depth tradeoff that no competitor has solved. We must teach agent patterns that are deep enough to be genuinely useful (not toy examples) while remaining accessible to people who have never configured a workflow tool. The no-code platform choice (n8n, Zapier) must be stable, learnable, and powerful enough for the exercises. If the platform is buggy, confusing, or too limited, the learning experience degrades regardless of curriculum quality.

- **About the competition:** No-code platforms must remain accessible to non-technical users. If n8n or similar platforms increase in complexity (adding features that make the UI harder for beginners), the teaching medium becomes an obstacle. Conversely, if vendor platforms (Copilot Studio) become so easy that "building an agent" is as simple as writing a prompt, then the training's hands-on differentiator is less necessary — anyone can do it without training.

**Confidence level:** Partially validated. The Udemy no-code AI agent ecosystem proves that non-technical users can build agents with tools like n8n (courses with 10,000+ students). But those courses are tool tutorials, not organizational transformation programs. Whether the combination — non-coder building + strategic framing + organizational scope — works is the central instructional design hypothesis. It is the hardest thing Agents 102 must deliver, and it is untested.

---

## Choice 11: Timing — "The window is 2026-2027"

Implicit throughout the business model is the assumption that the timing is right: the market is at an inflection point where enough companies are deploying agents that peer learning has value, but not so many that practices are codified and commoditized. The ai-network-curators research explicitly states the window may close in 12-18 months.

**What Would Have to Be True:**

- **About the customer:** Finnish mid-market companies must be entering their agentic transformation in 2026-2027, not 2028-2029. If the Finnish mid-market lags the global curve by 1-2 years (plausible for a small Nordic market), the training demand may not materialize until 2028 — by which time the window for building the network may have closed as larger players enter.

- **About the industry:** The agent deployment landscape must remain complex and unsettled long enough for the network to build critical mass. If vendor platforms simplify deployment so dramatically that "figuring it out together" is unnecessary, the network's value proposition evaporates. Conversely, if agent deployment stays too experimental and risky, companies may not commit to it at all — they wait for proven approaches, which the network cannot yet provide.

- **About the competition:** No well-resourced player must launch a competing network in the window. The research identifies the risk clearly: "In 12-18 months, this window may close as consultancies or platform vendors formalize their own practice frameworks." If Salesforce launches a curated Agentforce practice network, or if Gartner launches an AI Agent Peer Forum, they bring distribution and brand that a startup cannot match.

**Confidence level:** Partially validated. Gartner's prediction (40% of enterprise apps with agents by end of 2026, up from under 5% in 2025) validates that the inflection is happening. But whether this inflection is happening *in the Finnish mid-market* specifically is unknown. The timing bet is global-data-informed but locally unvalidated.

---

## Ranked Leap-of-Faith Assumptions

The following WWHTBT statements are both **the most uncertain** and **the most critical** to the business model's success. If any of these are wrong, the model needs fundamental revision. Ordered from most critical to least:

### 1. Companies will actively share their agentic deployment experiences in a structured peer network (Choice 3)
**Uncertainty: HIGH. Criticality: EXISTENTIAL.**
The entire business model depends on this. The network is the product, and the network only works if members contribute. No evidence exists that companies will share agentic practices — a domain they may view as competitively sensitive — in a structured, ongoing way. The IHI/CNCF analogs prove the model works in healthcare and open-source, but the transfer to commercial AI deployment is an untested leap. If companies join the network but treat it as a passive resource (consume but do not contribute), the curated practices library stays thin and the Phase 2 value proposition collapses.

### 2. The builder-leader buyer exists in sufficient numbers in the Finnish mid-market (Choice 1 + Choice 8)
**Uncertainty: HIGH. Criticality: HIGH.**
The psychographic is well-drawn but unvalidated. The Finnish mid-market is small. The intersection of "Finnish mid-market company" + "actively pursuing agentic transformation" + "has a builder-leader buyer" + "willing to try a new entrant over Aalto EE/Solita" could be a very small set. If the initial addressable market is 10-20 companies rather than 50-100, the Phase 1 economics may not work.

### 3. Non-technical learners can build agents meaningful enough to shift leadership's mental model (Choice 10 + Choice 2)
**Uncertainty: MEDIUM-HIGH. Criticality: HIGH.**
The "Week 2 demo" is the conversion moment — when leadership sees an agent working with their company's data and their thinking shifts. This requires non-technical learners to build something genuinely impressive, not a toy. The competitive research notes that nobody has cracked this because it is genuinely hard. If the hands-on exercises produce mediocre results, the "prerequisite" claim fails — leaders will conclude that agent competence requires developers, not trained business people.

### 4. Phase 1 training buyers will convert to Phase 2 network members (Choice 6)
**Uncertainty: MEDIUM-HIGH. Criticality: HIGH.**
The long-term business model depends on training buyers becoming network members. This requires a different internal justification (from "one-time training purchase" to "ongoing membership subscription"), a different budget line, and a different value assessment. Many organizations buy training once and move on. The conversion rate from Phase 1 to Phase 2 determines whether the business achieves network economics or remains a consulting-adjacent training shop.

### 5. Buyers prefer the curator model over the authority model for AI deployment guidance (Choice 5)
**Uncertainty: MEDIUM. Criticality: MEDIUM-HIGH.**
The positioning as "curator, not authority" is philosophically coherent but may not match how enterprise buyers actually make decisions. Many buyers want definitive answers backed by a trusted brand. "Here's how 12 companies handled it, each differently" may be intellectually honest but commercially less compelling than "here's the right answer, per Gartner." If the market gravitates toward prescriptive authority (a "SAFe for Agents"), the communal positioning becomes a niche play.

### 6. The Finnish mid-market is actively entering agentic transformation in 2026, not 2028 (Choice 11)
**Uncertainty: MEDIUM. Criticality: MEDIUM-HIGH.**
Global data supports the inflection, but Finnish mid-market timing is locally unvalidated. If Finnish companies are 12-18 months behind the US/UK curve, the training demand may not materialize until the window for building the network has passed.

### 7. The communal model will attract a larger market than a prescriptive model would (Choice 7)
**Uncertainty: MEDIUM. Criticality: MEDIUM.**
Historical evidence suggests prescriptive frameworks (SAFe, ITIL, PRINCE2) capture larger markets than communal ones — even when practitioners dislike them — because buyers want institutional cover and certifiable structure. The anti-SAFe positioning is principled but may leave the larger market to a future prescriptive competitor. This is a market-size risk, not a viability risk.

### 8. Aalto EE will not launch "good enough" agentic AI training before Agents 102 establishes itself (Choice 8)
**Uncertainty: MEDIUM. Criticality: MEDIUM.**
Aalto EE already has 6 AI programs and the Finnish L&D buyer relationship. If they launch an AI agents program (even a strategic/managerial one), they could absorb demand before Agents 102 reaches the market. The alternative — partnering with Aalto EE — turns this threat into an opportunity, but the partnership is not secured.

---

## Summary

The Agents 102 business model is built on a genuine market gap validated by extensive competitive research. No competitor combines hands-on agent building + strategic framing + non-coder accessibility + organizational scope + peer network curation. The model's architecture draws on proven analogs (IHI, CNCF, Vistage) and targets a real window of opportunity.

The critical uncertainties are not about competition or capability — they are about **desirability**. The model depends on buyers wanting something that has not been offered before and behaving in ways (sharing practices, converting to memberships, preferring curation over authority) that are hypothesized but unvalidated. The leap-of-faith assumptions cluster around three questions:

1. **Will companies share?** (the network contribution assumption)
2. **Will the builder-leader buy?** (the psychographic segment assumption)
3. **Will non-coders build well enough?** (the instructional design assumption)

These three assumptions are the ones most worth testing first — through design partner conversations, pilot cohort experiments, and prototype exercises — before investing heavily in the full business model build-out.
